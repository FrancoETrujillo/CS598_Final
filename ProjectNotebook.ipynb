{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:41.325931139Z",
     "start_time": "2024-04-14T02:09:41.155874844Z"
    }
   },
   "id": "c3a2a62280b2b116",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Improving Medical Predictions by Irregular Multimodal Electronic Health Records Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f954d686b0a84023"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "This paper pretends to address the challenges of handling irregularity and the integration of multimodal data for medical prediction tasks.\n",
    "\n",
    "## Background of the problem\n",
    "### What type of problem:\n",
    "The paper focuses on 2 main problems; Mortality Prediction and Phenotype Classification\n",
    "### What is the importance/meaning of solving the problem: \n",
    "ICUs admit patients with life-threatening conditions, Improving the efficacy and efficiency of predictions by accounting for irregular data in EHRs can help the medical providers to make more accurate and quick decisions that could save lives.\n",
    "\n",
    "### What is the difficulty of the problem:\n",
    "The primary difficulty is the handling the irregular sampling of data and the effective integration and modeling of EHR records like numerical time series and textual notes taken in multiple points in time and frequencies.\n",
    "\n",
    "![EHR sample image](.img/sample_ehr.png)\n",
    "\n",
    "### The state of the art methods and effectiveness.\n",
    "For irregular data handling;\n",
    "> [1] Lipton, Z. C., Kale, D., and Wetzel, R. Directly modeling\n",
    "> missing data in sequences with rnns: Improved classification of clinical time series. In Machine learning for\n",
    "> healthcare conference, pp. 253–270. PMLR, 2016.\n",
    "\n",
    "> [2] Shukla, S. N. and Marlin, B. M. Multi-time attention networks for irregularly sampled time series. arXiv preprint\n",
    "> arXiv:2101.10318, 2021.\n",
    "\n",
    "For irregular clinical notes processing;\n",
    "> [3] Golmaei, S. N. and Luo, X. Deepnote-gnn: predicting hospital readmission using clinical notes and patient network.\n",
    "> In Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics,\n",
    "> pp. 1–9, 2021.\n",
    "\n",
    "> [4]Mahbub, M., Srinivasan, S., Danciu, I., Peluso, A., Begoli, E., Tamang, S., and Peterson, G. D. \n",
    "> Unstructured clinical notes within the 24 hours since admission predict short,> mid & long-term mortality in adult icu patients. \n",
    "> Plos one, 17(1):e0262182, 2022.\n",
    "\n",
    "## Paper explanation\n",
    "### What did the paper propose\n",
    "The general problem addressed in this paper is to find a better approach to handling irregular multimodal data obtained on EHRs to better assess real-time predictions in ICUs. \n",
    "\n",
    "### What is the innovations of the method\n",
    "To better approach irregularity and multi-modal data the paper proposes integrating the real-time series and clinical notes while considering their irregularities. This by doing the following:\n",
    "\n",
    "![High level arch](.img/high_level_arch.png)\n",
    "\n",
    "#### Modeling Irregularity in Time Series:\n",
    "1. Temporal Discretization-Based Embeddings (TDE): Utilizes a novel unified\n",
    "approach (UTDE) that combines:\n",
    "    - Imputation: Regularizes time series by filling in missing values based\n",
    "on prior observations or statistical methods.\n",
    "    - Discretized Multi-Time Attention (mTAND): Applies a learned\n",
    "interpolation method using a multi-time attention mechanism to\n",
    "represent the irregular time series data better.\n",
    "2. Unified Approach (UTDE): This approach integrates imputation and mTAND\n",
    "through a gating mechanism to dynamically combine the representation of\n",
    "the time series.\n",
    " \n",
    " ![Detail arch](.img/imputation_plus_mtand.png)\n",
    "\n",
    "#### Processing Irregular Clinical Notes:\n",
    "1. Text Encoding: Uses a pretrained model (TextEncoder) to encode clinical\n",
    "notes into a series of representations.\n",
    "2. Irregularity Modeling: Sorts these representations by time, treats them as\n",
    "Multivariate Irregularly Sampled Time Series (MINSTS), and employs mTAND\n",
    "to generate a set of text interpolation representations to handle irregularities.\n",
    "\n",
    "\n",
    "#### Multimodal Fusion:\n",
    "1. Interleaved Attention Mechanism: Fuses time series and clinical note\n",
    "representations across temporal steps, integrating irregularity into multimodal\n",
    "representations.\n",
    "2. Self and Cross-Attention:\n",
    "    - Multi-Head Self-Attention (MH): Acquires contextual embeddings for\n",
    "each modality by focusing within the same modality across time.\n",
    "    - Multi-Head Cross-Attention (CMH): Each modality learns from the\n",
    "other, integrating information across modalities.\n",
    "3. Feed-Forward and Prediction Layers: A feed-forward sublayer follows the\n",
    "CMH outputs, with layer normalization and residual connections applied. The\n",
    "final step involves passing the integrated representations through fully\n",
    "connected layers to predict the outcome.\n",
    "\n",
    "\n",
    "### How well the proposed method work (in its own metrics)\n",
    " The proposed methods for two medical prediction tasks consistently outperforms state-ofthe-art (SOTA) baselines in each single modality and multimodal fusion scenarios. \n",
    "Observing a relative improvements of 6.5%, 3.6%, and 4.3% in F1 for time series, clinical notes, and multimodal fusion, respectively. \n",
    "\n",
    "### What is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n",
    "The paper's contribution is important because it provides a new direction for EHR-based predictive models to consider time irregularity that could lead to more accurate and reliable medical predictions, helping patients and healthcare processes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b51b40d117099b4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "For our project we plan to reproduce the experiment with In Hospital Mortality (IHM). And prove the following hypotheses:\n",
    "\n",
    "\n",
    "1. The inclusion of UTDE improves the performance of the model.\n",
    "2. Considering irregularities in clinical note embedding improves the performance of the model.\n",
    "3. The introduction of UTDE and mTAND for processing time series and clinical notes, respectively, plus the integration of Multimodal fusion outperforms F1 score against standard baselines."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f091ff27ef282204"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Methodology\n",
    "\n",
    "The project reproduction consists on the following sections\n",
    "- Data\n",
    "- Models\n",
    "- Training\n",
    "- Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f6d958f689e7c76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data\n",
    "\n",
    "This paper uses the MIMICIII dataset as starting point to obtain timeseries information and medical notes. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "302fccbbee93bdeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    " \n",
    "## Getting the data\n",
    " The following code contains some useful code to download and extract the dataset files locally\n",
    " \n",
    "**Note**: To download the mimic dataset is necessary to complete the request for access at [Physionet](https://physionet.org/)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18ea0197a327d87e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configs\n",
    "Open and edit GlobalConfigs.py to set up the local path to the project."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d55643c225c7bfa5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing mimic multimodal preprocess\n"
     ]
    }
   ],
   "source": [
    "# Imports and configs\n",
    "import subprocess\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import MultimodalMIMIC.preprocessing\n",
    "from GlobalConfigs import *\n",
    "\n",
    "DOWNLOAD_DATASET = False\n",
    "EXTRACT_COMPRESSED_CSVS = False\n",
    "PREPROCESS_BENCHMARKS = False\n",
    "PREPROCESS_CLINICAL_NOTES = False\n",
    "PREPROCESS_MULTIMODAL = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:42.043491241Z",
     "start_time": "2024-04-14T02:09:41.326367733Z"
    }
   },
   "id": "9e41a54007f74873",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# change physionet_username to your username\n",
    "if DOWNLOAD_DATASET:\n",
    "\n",
    "    physionet_username = \"your_user_name\"\n",
    "    password = \"your_pass\"\n",
    "    destination_directory = \"data/MIMICIII_Original\"\n",
    "\n",
    "    command = [\n",
    "        \"wget\", \"-r\", \"-N\", \"-c\", \"-np\",\n",
    "        \"--user\", physionet_username,\n",
    "        \"--password\", password,\n",
    "        \"https://physionet.org/files/mimiciii/1.4/\",\n",
    "        \"-P\", destination_directory\n",
    "    ]\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "\n",
    "    process.wait()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:42.044499944Z",
     "start_time": "2024-04-14T02:09:42.031622744Z"
    }
   },
   "id": "dd912cb01d3abcc0",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if EXTRACT_COMPRESSED_CSVS:\n",
    "    command = ['./decompress_mimic.sh', '-d', 'data/MIMICIII_Original/physionet.org/files/mimiciii/1.4/', '-o',\n",
    "               'data/mimic3']\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "\n",
    "    process.wait()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:42.045311295Z",
     "start_time": "2024-04-14T02:09:42.031767036Z"
    }
   },
   "id": "7b7101051b10d00a",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing the data\n",
    "The original paper leverages the following projects to help on the data preparation and extraction from the original MIMIC CSVs\n",
    "\n",
    "It leverages the **mimic3-benchmarks** and the **ClinicalNotesICU** for the following:\n",
    "\n",
    "- Cleanup invalid data\n",
    "- Map the events, diagnoses, and stays for each patient.\n",
    "- Extract timeseries for in-hospital-mortality \n",
    "- Split timeseries data into train and test sets\n",
    "- Extract Medical notes for patients\n",
    "- Split Medical notes for train and test sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13ee902ed9e8ef43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MIMIC benchmarks\n",
    "Helps to process timeseries data and divide train and test sets\n",
    "[mimic3-benchmarks](https://github.com/YerevaNN/mimic3-benchmarks.git)\n",
    "\n",
    "This repo contains a set of scripts that take the RAW mimic CSVs and prepare the irregular data:\n",
    "- extract_subjects.py:\n",
    "Generates one directory per SUBJECT_ID and writes ICU stay information to data/{SUBJECT_ID}/stays.csv, diagnoses to data/{SUBJECT_ID}/diagnoses.csv, and events to data/{SUBJECT_ID}/events.csv\n",
    "\n",
    "\n",
    "- validate_events.py\n",
    "Attempts to fix some issues (ICU stay ID is missing) and removes the events that have missing information. About 80% of events remain after removing all suspicious rows\n",
    "\n",
    "\n",
    "- extract_episodes_from_subjects.py\n",
    "Breaks up per-subject data into separate episodes (pertaining to ICU stays). Time series of events are stored in {SUBJECT_ID}/episode{#}_timeseries.csv (where # counts distinct episodes) while episode-level information (patient age, gender, ethnicity, height, weight) and outcomes (mortality, length of stay, diagnoses) are stores in {SUBJECT_ID}/episode{#}.csv. This script requires two files, one that maps event ITEMIDs to clinical variables and another that defines valid ranges for clinical variables\n",
    "\n",
    "\n",
    "- split_train_and_test.py\n",
    "Splits the whole dataset into training and testing sets.\n",
    "\n",
    "\n",
    "- create_in_hospital_mortality.py\n",
    "Generate task-specific datasets for in-hospital-mortality prediction\n",
    "\n",
    "After running the preparation scripts we end up with a directory data/in-hospital-mortality we have two subdirectories: train and test. Each of them contains a bunch of ICU stays and one file with name listfile.csv, which lists all samples in that particular set. Each row of listfile.csv has the following form: icu_stay, period_length, label(s). A row specifies a sample for which the input is the collection of ICU event of icu_stay that occurred in the first period_length hours of the stay and the target are label(s). In in-hospital mortality prediction task period_length is always 48 hours.\n",
    "\n",
    "\n",
    "The project does not work out of the box, so we downloaded the sourcecode and modify it inside this repo under the [mimic3-benchmarks](./mimic3-benchmarks) folder\n",
    "To simplify the process we have created the following script `./build_benchmark_data.sh` to run all timeseries required tasks.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c63cceb7ae9be99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if PREPROCESS_BENCHMARKS:\n",
    "    command = [\"./build_benchmark_data.sh\"]\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "    process.wait()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:42.046604856Z",
     "start_time": "2024-04-14T02:09:42.031859871Z"
    }
   },
   "id": "572e454792dbbde4",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ClinicalNotesICU\n",
    "Helps to process medical notes and divide in train and test\n",
    " [ClinicalNotesICU](https://github.com/kaggarwal/ClinicalNotesICU.git)\n",
    "\n",
    "Similar to the mimic3-benchmarks, this repo contains a set of scripts that take the RAW mimic CSVs and process the clinical notes for the previously generated train and test datasets.\n",
    "\n",
    "- extract_notes.py\n",
    "Uses the NOTEEVENTS.csv and the previously generated train and test sets to extract the notes within the first 48 hours of the event and saves them on its own train a test data directories \n",
    "\n",
    "- extract_T0.py\n",
    "Uses the stays.csv and events.csv to extract the episodes start time and save them into a binary pkl file.\n",
    "\n",
    "The project does not work out of the box, so we downloaded the sourcecode and modify it inside this repo under the [ClinicalNotesICU](./ClinicalNotesICU) folder\n",
    "\n",
    "To simplify the process we have created the following script `./extract_med_notes.sh` tu run all the required tasks for clinical notes.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44f45bd0bc84d872"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if PREPROCESS_CLINICAL_NOTES:\n",
    "    command = [\"./extract_med_notes.sh\"]\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "    process.wait()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:42.146022047Z",
     "start_time": "2024-04-14T02:09:42.043611448Z"
    }
   },
   "id": "d23722350fecd73f",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess time series For Mimic Multimodal \n",
    "The next step is to discretize and normalize the timeseries data, as well as link the clinical notes with their corresponding timestamps.\n",
    "\n",
    "The [paper's repo](https://github.com/XZhang97666/MultimodalMIMIC.git) provides a preprocessing script to work on this task.\n",
    "\n",
    "After running the preprocessing steps we save the following PKLs to be used by the model: \n",
    "```\n",
    "mean_std.pkl \n",
    "norm_ts_test.pkl\n",
    "norm_ts_train.pkl\n",
    "norm_ts_val.pkl\n",
    "testp2x_data.pkl\n",
    "trainp2x_data.pkl\n",
    "ts_test.pkl\n",
    "ts_train.pkl\n",
    "ts_val.pkl\n",
    "valp2x_data.pkl\n",
    "```\n",
    "The project does not work out of the box, so we used it partially to call some of the functions from this notebook; the downloaded sourcecode and modifications are inside this repo under the [MultimodalMIMIC](./MultimodalMIMIC) folder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "839edc8722b2b0e5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from mimic3benchmark.readers import InHospitalMortalityReader\n",
    "from MultimodalMIMIC.preprocessing import Discretizer_multi\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Any\n",
    "from readers import Reader\n",
    "import gzip\n",
    "from mimic3models.preprocessing import Normalizer\n",
    "from MultimodalMIMIC.preprocessing import extract_irregular\n",
    "from MultimodalMIMIC.preprocessing import mean_std\n",
    "from MultimodalMIMIC.preprocessing import normalize\n",
    "\n",
    "\n",
    "# Paths for data\n",
    "ihm_data_path = f\"{BENCHMARKS_ROOT_PATH}/data/in-hospital-mortality\"\n",
    "ihm_train_data_path = f\"{ihm_data_path}/train\"\n",
    "ihm_test_data_path = f\"{ihm_data_path}/test\"\n",
    "discretizer_config_path = f\"{MULTI_MODAL_MIMIC_PATH}/Data/irregular/discretizer_config.json\"\n",
    "channel_info_path = f\"{MULTI_MODAL_MIMIC_PATH}/Data/irregular/channel_info.json\"\n",
    "textdata_fixed = f\"{BENCHMARKS_ROOT_PATH}/data/root/text_fixed/train/\"\n",
    "text_start_time_path = f\"{BENCHMARKS_ROOT_PATH}/data/root/text_fixed/starttime.pkl\"\n",
    "test_textdata_fixed = f\"{BENCHMARKS_ROOT_PATH}/data/root/text_fixed/test/\"\n",
    "test_text_start_time_path = f\"{BENCHMARKS_ROOT_PATH}/data/root/text_fixed/test_starttime.pkl\"\n",
    "ihm_discrete_save_path = f\"{PROJECT_BASE_PATH}/Data/ihm/\"\n",
    "\n",
    "# Modify this to take only a subset of the full data; None takes the full data\n",
    "n_samples_elements = 1000\n",
    "\n",
    "mortality_period = 48\n",
    "timestep = 1.0\n",
    "imputation = \"previous\"\n",
    "dataset_types = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "normalizer_state_file_path = f'{BENCHMARKS_ROOT_PATH}/mimic3models/in_hospital_mortality/ihm_ts{timestep}.input_str-{imputation}.start_time-zero.normalizer'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:42.157880255Z",
     "start_time": "2024-04-14T02:09:42.087662287Z"
    }
   },
   "id": "1ed034beed47eeaf",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading the data\n",
    "First define the InHospitalMortalityReader to help us easily access the split datasets generated by mimic3-benchmarks. It provides helper functions to read multiple data series samples and labels by patient and map to a dictionary."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c972d061613ed073"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_reader = InHospitalMortalityReader(dataset_dir=ihm_train_data_path,\n",
    "                                         listfile=os.path.join(ihm_train_data_path, 'listfile.csv'),\n",
    "                                         period_length=mortality_period)\n",
    "val_reader = InHospitalMortalityReader(dataset_dir=ihm_train_data_path,\n",
    "                                       listfile=os.path.join(ihm_train_data_path, 'listfile.csv'),\n",
    "                                       period_length=mortality_period)\n",
    "\n",
    "test_reader = InHospitalMortalityReader(dataset_dir=ihm_test_data_path,\n",
    "                                        listfile=os.path.join(ihm_test_data_path, 'listfile.csv'),\n",
    "                                        period_length=mortality_period)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:42.206837021Z",
     "start_time": "2024-04-14T02:09:42.087812410Z"
    }
   },
   "id": "2f0db8b42d1f1ab5",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def save_compressed_pkl_gz(data_to_dump: Any, save_name: str):\n",
    "    print(\"saving and compressing:\", save_name)\n",
    "    with gzip.open(f'{save_name}.pkl.gz', 'wb') as file:\n",
    "        pickle.dump(data_to_dump, file)\n",
    "\n",
    "\n",
    "def load_compressed_pkl(full_file_name: str) -> Any:\n",
    "    with gzip.open(full_file_name, 'rb') as file:\n",
    "        data_loaded = pickle.load(file)\n",
    "    return data_loaded\n",
    "\n",
    "\n",
    "def read_chunk(reader, chunk_size):\n",
    "    chunk_data = {}\n",
    "    for _ in tqdm(range(chunk_size), desc=\"reading data\"):\n",
    "        ret = reader.read_next()\n",
    "        for k, v in ret.items():\n",
    "            if k not in chunk_data:\n",
    "                chunk_data[k] = []\n",
    "            chunk_data[k].append(v)\n",
    "    chunk_data[\"header\"] = chunk_data[\"header\"][0]\n",
    "    return chunk_data\n",
    "\n",
    "\n",
    "def discretize_and_save_data(reader: Reader, discretizer: Discretizer_multi, save_path: str,\n",
    "                             partial_n_samples: Optional[int] = None, save_name=str, compress_pkl: bool = False):\n",
    "    n_samples = reader.get_number_of_examples()\n",
    "    if partial_n_samples:\n",
    "        n_samples = partial_n_samples\n",
    "    ret = read_chunk(reader, n_samples)\n",
    "    irg_data = ret[\"X\"]\n",
    "    ts = ret[\"t\"]\n",
    "    labels = ret[\"y\"]\n",
    "    discrete_names = ret[\"name\"]\n",
    "\n",
    "    reg_data = []\n",
    "    for X, t in tqdm(zip(irg_data, ts), total=len(irg_data), desc=f\"discretizing data \"):\n",
    "        transformed_data = discretizer.transform(X, end=t)[0]\n",
    "        reg_data.append(transformed_data)\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    save_full_path = save_path + save_name\n",
    "    if compress_pkl:\n",
    "        save_compressed_pkl_gz((irg_data, reg_data, labels, discrete_names), save_full_path)\n",
    "    else:\n",
    "        print(\"Saving\", f\"{save_full_path}.pkl\")\n",
    "        with open(f\"{save_full_path}.pkl\", 'wb') as file:\n",
    "            pickle.dump((irg_data, reg_data, labels, discrete_names), file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:42.235867094Z",
     "start_time": "2024-04-14T02:09:42.177749310Z"
    }
   },
   "id": "e05696cbeab2d89a",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discretize temporal data and add imputation\n",
    "\n",
    "In order to create the embeddings for the proposed model first we need to discretize and add the imputation to the missing values. In our case we will use imputation using the previous value of the series.\n",
    "\n",
    "The Discretizer_multi takes care of transforming the irregular data into samples at each timestep while filling the missing data with the desired imputation strategy considering all time based features.\n",
    "\n",
    "![Discretizer image](.img/only_discretize.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ba45dd4edd24c95"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discretize and save train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading data: 100%|██████████| 1000/1000 [00:00<00:00, 1304.15it/s]\n",
      "discretizing data : 100%|██████████| 1000/1000 [00:02<00:00, 371.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/ts_train.pkl\n",
      "discretize and save val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading data: 100%|██████████| 1000/1000 [00:00<00:00, 2026.71it/s]\n",
      "discretizing data : 100%|██████████| 1000/1000 [00:01<00:00, 543.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/ts_val.pkl\n",
      "discretize and save test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading data: 100%|██████████| 1000/1000 [00:00<00:00, 2019.10it/s]\n",
      "discretizing data : 100%|██████████| 1000/1000 [00:01<00:00, 576.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/ts_test.pkl\n"
     ]
    }
   ],
   "source": [
    "discretizer_multi = Discretizer_multi(\n",
    "    impute_strategy='previous',\n",
    "    store_masks=True,\n",
    "    start_time='zero',\n",
    "    config_path=discretizer_config_path,\n",
    "    channel_path=channel_info_path\n",
    ")\n",
    "discretizer_header = discretizer_multi.transform(train_reader.read_example(0)[\"X\"])[1].split(',')\n",
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "# print(\"discretizer_header\", discretizer_header)\n",
    "# print(\"cont_channels\", cont_channels)\n",
    "\n",
    "print(\"discretize and save train\")\n",
    "discretize_and_save_data(train_reader, discretizer_multi, ihm_discrete_save_path, n_samples_elements, \"ts_train\")\n",
    "print(\"discretize and save val\")\n",
    "discretize_and_save_data(val_reader, discretizer_multi, ihm_discrete_save_path, n_samples_elements, \"ts_val\")\n",
    "print(\"discretize and save test\")\n",
    "discretize_and_save_data(test_reader, discretizer_multi, ihm_discrete_save_path, n_samples_elements, \"ts_test\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:52.426451715Z",
     "start_time": "2024-04-14T02:09:42.219959214Z"
    }
   },
   "id": "e5be5a812eb8f9ec",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Paddings and masks\n",
    "After discretize we need to apply paddings and create masks for all features. For this we can use the function extract_irregular from MultimodalMimic. It creates the padded irregular and mask arrays and save them for later use. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3f129af1f21d395"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train irregular data\n",
      "Saving: /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/ts_train.pkl\n",
      "Extracting val irregular data\n",
      "Saving: /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/ts_val.pkl\n",
      "Extracting test irregular data\n",
      "Saving: /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/ts_test.pkl\n"
     ]
    }
   ],
   "source": [
    "for dataset_type in dataset_types:\n",
    "    print(f\"Extracting {dataset_type} irregular data\", flush=True)\n",
    "    in_extract_data_path = ihm_discrete_save_path + 'ts_' + dataset_type + '.pkl'\n",
    "    out_extract_data_path = ihm_discrete_save_path + 'ts_' + dataset_type + '.pkl'\n",
    "    extract_irregular(in_extract_data_path, out_extract_data_path, channel_info_path, discretizer_config_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:56.093900210Z",
     "start_time": "2024-04-14T02:09:52.427574053Z"
    }
   },
   "id": "72092e5bc118117e",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/mean_std.pkl\n"
     ]
    }
   ],
   "source": [
    "mean_std(ihm_discrete_save_path + 'ts_train.pkl', ihm_discrete_save_path + 'mean_std.pkl')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:09:57.360790572Z",
     "start_time": "2024-04-14T02:09:56.093225136Z"
    }
   },
   "id": "acce6019f9e94910",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalizing timeseries data\n",
    "Now we apply normalization to our data by x = (x - means[f_idx]) / stds[f_idx], For this, we leverage the Normalizer provided by the mimic3-benchmarks repo\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "407668b846604ce6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing train times data\n",
      "Saving: /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/norm_ts_train.pkl\n",
      "Normalizing val times data\n",
      "Saving: /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/norm_ts_val.pkl\n",
      "Normalizing test times data\n",
      "Saving: /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/norm_ts_test.pkl\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer(fields=cont_channels)\n",
    "normalizer.load_params(normalizer_state_file_path)\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    print(f\"Normalizing {dataset_type} times data\", flush=True)\n",
    "    normalize(ihm_discrete_save_path + 'ts_' + dataset_type + '.pkl',\n",
    "              ihm_discrete_save_path + 'norm_ts_' + dataset_type + '.pkl',\n",
    "              ihm_discrete_save_path + 'mean_std.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:10:00.661194523Z",
     "start_time": "2024-04-14T02:09:57.359620644Z"
    }
   },
   "id": "3b6d118f0c2f7865",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the Text data\n",
    "We finally prepare the text data within a period (48 in our case) and generate a json containing the note as well as the time until the end of the period. For this we use the TextReader helper from the MultimodalMIMIC module."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef855063840a6659"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing  train text data\n",
      "Suceed Merging:  750\n",
      "Missing Merging:  250\n",
      "File dumped at: /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/trainp2x_data.pkl\n",
      "Preparing  val text data\n",
      "Suceed Merging:  750\n",
      "Missing Merging:  250\n",
      "File dumped at: /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/valp2x_data.pkl\n",
      "Preparing  test text data\n",
      "Suceed Merging:  762\n",
      "Missing Merging:  238\n",
      "File dumped at: /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/testp2x_data.pkl\n"
     ]
    }
   ],
   "source": [
    "from MultimodalMIMIC.text_utils import TextReader\n",
    "from MultimodalMIMIC.preprocessing import merge_text_ts\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    print(f\"Preparing  {dataset_type} text data\", flush=True)\n",
    "\n",
    "    with open(ihm_discrete_save_path + 'norm_ts_' + dataset_type + '.pkl', 'rb') as f:\n",
    "        tsdata = pickle.load(f)\n",
    "\n",
    "    names = [data['name'] for data in tsdata]\n",
    "\n",
    "    if (dataset_type == 'train') or (dataset_type == 'val'):\n",
    "        text_reader = TextReader(textdata_fixed, text_start_time_path)\n",
    "    else:\n",
    "        text_reader = TextReader(test_textdata_fixed, test_text_start_time_path)\n",
    "\n",
    "    data_text, data_times, data_time = text_reader.read_all_text_append_json(names, mortality_period)\n",
    "    merge_text_ts(data_text, data_times, data_time, tsdata, mortality_period,\n",
    "                  ihm_discrete_save_path + dataset_type + 'p2x_data.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:10:03.055279449Z",
     "start_time": "2024-04-14T02:10:00.663788959Z"
    }
   },
   "id": "ae28b8cd4f293522",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:10:03.058803940Z",
     "start_time": "2024-04-14T02:10:03.055257498Z"
    }
   },
   "id": "4b02786a1c48328f",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read preprocessed PKLs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2361f279fd3d555d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:10:03.067192860Z",
     "start_time": "2024-04-14T02:10:03.058605706Z"
    }
   },
   "id": "b30176d3e76d1b65",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 6328\n",
      "train: 35948\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clinical_notes_path = f\"{BENCHMARKS_ROOT_PATH}/data/root/text_fixed\"\n",
    "\n",
    "# Start times pkl contains a map of patient_id notes with its recorded time\n",
    "test_note_start_time = f\"{clinical_notes_path}/test_starttime.pkl\"\n",
    "train_note_start_time = f\"{clinical_notes_path}/starttime.pkl\"\n",
    "\n",
    "test_start_times_dict = {}\n",
    "train_start_times_dict = {}\n",
    "with open(test_note_start_time, 'rb') as f:\n",
    "    test_start_times_dict.update(pickle.load(f))\n",
    "\n",
    "with open(train_note_start_time, 'rb') as f:\n",
    "    train_start_times_dict.update(pickle.load(f))\n",
    "\n",
    "print(\"test:\", len(test_start_times_dict))\n",
    "print(\"train:\", len(train_start_times_dict))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:10:03.090020377Z",
     "start_time": "2024-04-14T02:10:03.063723313Z"
    }
   },
   "id": "6e7d3cd1d20c29c4",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test texts: 6107\n",
      "train texts: 34748\n",
      "/media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/mimic3-benchmarks/data/root/text_fixed/test/10000_1\n"
     ]
    }
   ],
   "source": [
    "text_train_files = []\n",
    "text_test_files = []\n",
    "\n",
    "text_train_filepath = f\"{clinical_notes_path}/train\"\n",
    "text_test_filepath = f\"{clinical_notes_path}/test\"\n",
    "\n",
    "with os.scandir(text_train_filepath) as entries:\n",
    "    text_train_files.extend([entry.name for entry in entries if entry.is_file() and entry.name[0].isdigit()])\n",
    "\n",
    "with os.scandir(text_test_filepath) as entries:\n",
    "    text_test_files.extend([entry.name for entry in entries if entry.is_file() and entry.name[0].isdigit()])\n",
    "\n",
    "print(\"test texts:\", len(text_test_files))\n",
    "print(\"train texts:\", len(text_train_files))\n",
    "print(f\"{text_test_filepath}/{text_test_files[0]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:10:03.141967985Z",
     "start_time": "2024-04-14T02:10:03.093083207Z"
    }
   },
   "id": "aa7eccdffa04299b",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Open PKL\n",
    "dataPath = f\"{ihm_discrete_save_path}p2x_data.pkl\"\n",
    "if os.path.isfile(dataPath):\n",
    "    print('Using', dataPath)\n",
    "    with open(dataPath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        print(\"pkl data:\", data[0].keys())\n",
    "        \n",
    "    print(data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:10:03.156449422Z",
     "start_time": "2024-04-14T02:10:03.143041611Z"
    }
   },
   "id": "246aeb4f99d4a8d",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:10:03.159061671Z",
     "start_time": "2024-04-14T02:10:03.156952842Z"
    }
   },
   "id": "b5329aa066afdc71",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Franco Trying to load: /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/Data/ihm/p2x_data.pkl\n",
      "Using /media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/MultimodalMIMIC/run/TS_Text/ihm/TS_Text/TS_48/Atten/Text_48/bioLongformer/512/cross_attn3/batch/irregular_TS_64/irregular_Text_64/2e-05_2_3_0.0004_1_8_128_1_2/result.pkl\n",
      "result {42: {'auc': {'val': 0.8444374825953774, 'test': 0.7999665775401069}, 'auprc': {'val': 0.5019722727999508, 'test': 0.3680000185874538}, 'f1': {'val': 0.371859296482412, 'test': 0.28220858895705525}}}\n"
     ]
    }
   ],
   "source": [
    "file_name = \"/media/ftrujillo/FRD/Projects/UIUC/DLH/CS598_Final/MultimodalMIMIC/run/TS_Text/ihm/TS_Text/TS_48/Atten/Text_48/bioLongformer/512/cross_attn3/batch/irregular_TS_64/irregular_Text_64/2e-05_2_3_0.0004_1_8_128_1_2/result.pkl\"\n",
    "print(\"Franco\", f\"Trying to load: {dataPath}\")\n",
    "\n",
    "if os.path.isfile(file_name):\n",
    "    print('Using', file_name)\n",
    "    with open(file_name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "print(\"result\", data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:10:03.203906676Z",
     "start_time": "2024-04-14T02:10:03.159553359Z"
    }
   },
   "id": "a404dc909a7ece9e",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--task TASK] [--file_path FILE_PATH]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "                             [--tensorboard_dir TENSORBOARD_DIR] [--seed SEED]\n",
      "                             [--mode MODE] [--modeltype MODELTYPE]\n",
      "                             [--eval_score EVAL_SCORE]\n",
      "                             [--num_labels NUM_LABELS]\n",
      "                             [--max_length MAX_LENGTH] [--pad_to_max_length]\n",
      "                             [--model_path MODEL_PATH]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "                             [--num_update_bert_epochs NUM_UPDATE_BERT_EPOCHS]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--txt_learning_rate TXT_LEARNING_RATE]\n",
      "                             [--ts_learning_rate TS_LEARNING_RATE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
      "                             [--pt_mask_ratio PT_MASK_RATIO]\n",
      "                             [--mean_mask_length MEAN_MASK_LENGTH] [--chunk]\n",
      "                             [--chunk_type CHUNK_TYPE]\n",
      "                             [--warmup_proportion WARMUP_PROPORTION]\n",
      "                             [--kernel_size KERNEL_SIZE]\n",
      "                             [--num_heads NUM_HEADS] [--layers LAYERS]\n",
      "                             [--cross_layers CROSS_LAYERS]\n",
      "                             [--embed_dim EMBED_DIM]\n",
      "                             [--irregular_learn_emb_ts]\n",
      "                             [--irregular_learn_emb_text] [--reg_ts]\n",
      "                             [--tt_max TT_MAX] [--embed_time EMBED_TIME]\n",
      "                             [--ts_to_txt] [--txt_to_ts] [--dropout DROPOUT]\n",
      "                             [--model_name MODEL_NAME]\n",
      "                             [--num_of_notes NUM_OF_NOTES]\n",
      "                             [--notes_order NOTES_ORDER]\n",
      "                             [--ratio_notes_order RATIO_NOTES_ORDER]\n",
      "                             [--bertcount BERTCOUNT]\n",
      "                             [--first_n_item FIRST_N_ITEM] [--fine_tune]\n",
      "                             [--self_cross] [--TS_mixup]\n",
      "                             [--mixup_level MIXUP_LEVEL] [--fp16] [--debug]\n",
      "                             [--generate_data] [--FTLSTM] [--Interp] [--cpu]\n",
      "                             [--datagereate_seed DATAGEREATE_SEED]\n",
      "                             [--TS_model TS_MODEL]\n",
      "                             [--cross_method CROSS_METHOD]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/ftrujillo/.local/share/jupyter/runtime/kernel-2517a226-3324-4104-9022-fbc47229a9f1.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ftrujillo/miniconda3/envs/DLH/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from MultimodalMIMIC.main import main\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:10:05.103667536Z",
     "start_time": "2024-04-14T02:10:03.203704915Z"
    }
   },
   "id": "c2b67ed89a87d876",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea129f0ca6817b1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-14T02:10:05.098877147Z"
    }
   },
   "id": "bd36e4457baa5aa5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2abaeb54759b9c2f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-14T02:10:05.101783100Z"
    }
   },
   "id": "26139a98645604c7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ablation 1 - Drop UTDE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b35e085c337f3375"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T02:10:05.106827669Z",
     "start_time": "2024-04-14T02:10:05.104455262Z"
    }
   },
   "id": "91ee0bc909bde8c6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ablation 2 - Remove mTAND"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50a006bd896d6e70"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-14T02:10:05.110253454Z"
    }
   },
   "id": "9c7df7867cf3098e",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
