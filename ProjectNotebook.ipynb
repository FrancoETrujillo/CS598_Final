{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f954d686b0a84023",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Improving Medical Predictions by Irregular Multimodal Electronic Health Records Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2cda9f6d05930",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Team and Repo\n",
    "## Team Members:\n",
    "- Franco E.Trujillo - fet2@illinois.edu\n",
    "- Hongyi Wu - hongyiw6@illinois.edu\n",
    "\n",
    "## Project Repo:\n",
    "- [https://github.com/FrancoETrujillo/CS598_Final](https://github.com/FrancoETrujillo/CS598_Final)\n",
    "\n",
    "## Reference Repos:\n",
    "- [https://github.com/XZhang97666/MultimodalMIMIC](https://github.com/XZhang97666/MultimodalMIMIC)\n",
    "    - commit hash used: 8e513ca\n",
    "- [https://github.com/YerevaNN/mimic3-benchmarks](https://github.com/YerevaNN/mimic3-benchmarks)\n",
    "    - commit hash used: ea0314c\n",
    "- [https://github.com/kaggarwal/ClinicalNotesICU](https://github.com/kaggarwal/ClinicalNotesICU)\n",
    "    - commit hash used: 9c2b740"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b40d117099b4b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "This paper intends to address the challenges of handling irregularity and the integration of multimodal data for medical prediction tasks.\n",
    "\n",
    "## Background of the problem\n",
    "### What type of problem:\n",
    "The paper focuses on 2 main problems; Mortality Prediction and Phenotype Classification\n",
    "### What is the importance/meaning of solving the problem: \n",
    "ICUs admit patients with life-threatening conditions, Improving the efficacy and efficiency of predictions by accounting for irregular data in EHRs can help the medical providers to make more accurate and quick decisions that could save lives.\n",
    "\n",
    "### What is the difficulty of the problem:\n",
    "The primary difficulty is the handling the irregular sampling of data and the effective integration and modeling of EHR records like numerical time series and textual notes taken in multiple points in time and frequencies.\n",
    "\n",
    "![EHR sample image](.img/sample_ehr.png)\n",
    "\n",
    "### The state-of-the-art methods and effectiveness.\n",
    "For irregular data handling;\n",
    "> [1] Lipton, Z. C., Kale, D., and Wetzel, R. Directly modeling\n",
    "> missing data in sequences with rnns: Improved classification of clinical time series. In Machine learning for\n",
    "> healthcare conference, pp. 253–270. PMLR, 2016.\n",
    "\n",
    "> [2] Shukla, S. N. and Marlin, B. M. Multi-time attention networks for irregularly sampled time series. arXiv preprint\n",
    "> arXiv:2101.10318, 2021.\n",
    "\n",
    "For irregular clinical notes processing;\n",
    "> [3] Golmaei, S. N. and Luo, X. Deepnote-gnn: predicting hospital readmission using clinical notes and patient network.\n",
    "> In Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics,\n",
    "> pp. 1–9, 2021.\n",
    "\n",
    "> [4]Mahbub, M., Srinivasan, S., Danciu, I., Peluso, A., Begoli, E., Tamang, S., and Peterson, G. D. \n",
    "> Unstructured clinical notes within the 24 hours since admission predict short,> mid & long-term mortality in adult icu patients. \n",
    "> Plos one, 17(1):e0262182, 2022.\n",
    "\n",
    "## Paper explanation\n",
    "### What did the paper propose\n",
    "The general problem addressed in this paper is to find a better approach to handling irregular multimodal data obtained on EHRs to better assess real-time predictions in ICUs. \n",
    "\n",
    "### What is the innovations of the method\n",
    "To better approach irregularity and multi-modal data the paper proposes integrating the real-time series and clinical notes while considering their irregularities. This by doing the following:\n",
    "\n",
    "![High level arch](.img/high_arch_w_desc.png)\n",
    "\n",
    "#### Modeling Irregularity in Time Series:\n",
    "1. Temporal Discretization-Based Embeddings (TDE): Utilizes a novel unified\n",
    "approach (UTDE) that combines:\n",
    "    - Imputation: Regularizes time series by filling in missing values based\n",
    "on prior observations or statistical methods.\n",
    "    - Discretized Multi-Time Attention (mTAND): Applies a learned\n",
    "interpolation method using a multi-time attention mechanism to\n",
    "represent the irregular time series data better.\n",
    "2. Unified Approach (UTDE): This approach integrates imputation and mTAND\n",
    "through a gating mechanism to dynamically combine the representation of\n",
    "the time series.\n",
    " \n",
    " ![Detail arch](.img/imputation_plus_mtand.png)\n",
    "\n",
    "#### Processing Irregular Clinical Notes:\n",
    "1. Text Encoding: Uses a pretrained model (TextEncoder) to encode clinical\n",
    "notes into a series of representations.\n",
    "2. Irregularity Modeling: Sorts these representations by time, treats them as\n",
    "Multivariate Irregularly Sampled Time Series (MINSTS), and employs mTAND\n",
    "to generate a set of text interpolation representations to handle irregularities.\n",
    "\n",
    "\n",
    "#### Multimodal Fusion:\n",
    "1. Interleaved Attention Mechanism: Fuses time series and clinical note\n",
    "representations across temporal steps, integrating irregularity into multimodal\n",
    "representations.\n",
    "2. Self and Cross-Attention:\n",
    "    - Multi-Head Self-Attention (MH): Acquires contextual embeddings for\n",
    "each modality by focusing within the same modality across time.\n",
    "    - Multi-Head Cross-Attention (CMH): Each modality learns from the\n",
    "other, integrating information across modalities.\n",
    "3. Feed-Forward and Prediction Layers: A feed-forward sublayer follows the\n",
    "CMH outputs, with layer normalization and residual connections applied. The\n",
    "final step involves passing the integrated representations through fully\n",
    "connected layers to predict the outcome.\n",
    "\n",
    "\n",
    "### How well the proposed method work (in its own metrics)\n",
    " The proposed methods for two medical prediction tasks consistently outperforms state-ofthe-art (SOTA) baselines in each single modality and multimodal fusion scenarios. \n",
    "Observing a relative improvements of 6.5%, 3.6%, and 4.3% in F1 for time series, clinical notes, and multimodal fusion, respectively. \n",
    "\n",
    "### What is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n",
    "The paper's contribution is important because it provides a new direction for EHR-based predictive models to consider time irregularity that could lead to more accurate and reliable medical predictions, helping patients and healthcare processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091ff27ef282204",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "For our project we plan to reproduce the experiment with In Hospital Mortality (IHM). And prove the following hypotheses:\n",
    "\n",
    "\n",
    "1. The inclusion of UTDE improves the performance of the model.\n",
    "2. Considering irregularities in clinical note embedding improves the performance of the model.\n",
    "3. The introduction of UTDE and mTAND for processing time series and clinical notes, respectively, plus the integration of Multimodal fusion outperforms F1 score against standard baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaf5f852216c24d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prerequisites to Reproduce the project\n",
    "- Get access to the MIMIC dataset\n",
    "- Modify the GlobalConfigs.py to use your own project and data paths\n",
    "- Install the required dependencies listed on Requirements.txt, we recommend using Conda with python 3.11\n",
    "- Modify the directory variables on the **Configuring imports and directories** section bellow if needed\n",
    "\n",
    "**Notes:** \n",
    "\n",
    "- We are unable to share the preprocess pkl files due to the [MIMIC DUA](https://physionet.org/content/mimiciii/view-dua/1.4/)\n",
    "- This project has being developed and tested using Linux Mint 21.2, ubuntu variants should work, but you may need to modify it to execute on another OS\n",
    "- More information about our dir structure can be found on our README.md \n",
    "- For this project we've used an NVDIA 4070 GPU with a limited dataset, you may need more resources for your case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d958f689e7c76",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Methodology\n",
    "\n",
    "The project reproduction consists on the following sections\n",
    "- Data\n",
    "- Models\n",
    "- Training\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fccbbee93bdeb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data\n",
    "\n",
    "This paper uses the MIMICIII dataset as starting point to obtain timeseries information and medical notes. \n",
    "\n",
    "The MIMIC-III dataset is composed of a set of CSV files containing information about patients, their stays, events, and notes. \n",
    "\n",
    "For our project the most relevant tables are:\n",
    "### ADMISSIONS\n",
    "Contains information about the admissions of patients to the hospital.\n",
    "\n",
    "![Admissions](.img/Addmissions_table.png)\n",
    "\n",
    "\n",
    "### PATIENTS\n",
    "Contains information about the patients.\n",
    "\n",
    "![Patients](.img/patients_table.png)\n",
    "\n",
    "\n",
    "### ICUSTAYS\n",
    "Contains information about the ICU stays of patients.\n",
    "\n",
    "![ICUStays](.img/icu_stays_table.png)\n",
    "\n",
    "\n",
    "### NOTEEVENTS\n",
    "Contains information about the notes taken for each patient.\n",
    "\n",
    "![NoteEvents](.img/NoteEvents_table.png)\n",
    "\n",
    "\n",
    "### CALLOUT\n",
    "Contains information about when patients were ready for discharge (called out), and the actual time of their discharge (or more generally, their outcome).\n",
    "\n",
    "![Callout](.img/callout_table.png)\n",
    "\n",
    "\n",
    "More info about the table structures can be found at [https://mit-lcp.github.io/mimic-schema-spy/index.html](https://mit-lcp.github.io/mimic-schema-spy/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea0197a327d87e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " \n",
    "## Getting the data\n",
    " The following code contains some useful code to download and extract the dataset files locally\n",
    " \n",
    "**Note**: To download the mimic dataset is necessary to complete the request for access at [Physionet](https://physionet.org/)\n",
    "\n",
    "After downloading and extracting the dataset, we will have a directory structure like this:\n",
    "```\n",
    "├── ClinicalNotesICU\n",
    "│   ├── models\n",
    "│   └── scripts\n",
    "├── mimic3-benchmarks\n",
    "│   ├── data\n",
    "│   │   ├── decompensation\n",
    "│   │   ├── in-hospital-mortality\n",
    "│   │   ├── length-of-stay\n",
    "│   │   ├── multitask\n",
    "│   │   ├── phenotyping\n",
    "│   │   └── root\n",
    "│   │       ├── test_text_fixed\n",
    "│   │       └── text_fixed\n",
    "│   ├── mimic3benchmark\n",
    "│   │   ├── evaluation\n",
    "│   │   ├── resources\n",
    "│   │   ├── scripts\n",
    "│   │   └── tests\n",
    "│   │       └── resources\n",
    "│   └── mimic3models\n",
    "│       ├── decompensation\n",
    "│       │   └── logistic\n",
    "│       ├── in_hospital_mortality\n",
    "│       │   └── logistic\n",
    "│       ├── keras_models\n",
    "│       ├── length_of_stay\n",
    "│       │   └── logistic\n",
    "│       ├── multitask\n",
    "│       ├── phenotyping\n",
    "│       │   └── logistic\n",
    "│       └── resources\n",
    "└── MultimodalMIMIC\n",
    "    ├── Data\n",
    "    │   ├── ihm\n",
    "    │   └── irregular\n",
    "    └── run\n",
    "        └── TS_Text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55643c225c7bfa5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Config tasks to execute\n",
    "Open and edit GlobalConfigs.py to set up the local path to the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e41a54007f74873",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "# Imports and configs\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "from GlobalConfigs import *\n",
    "\n",
    "DOWNLOAD_DATASET = False\n",
    "EXTRACT_COMPRESSED_CSVS = False\n",
    "PREPROCESS_BENCHMARKS = False\n",
    "PREPROCESS_CLINICAL_NOTES = False\n",
    "PREPROCESS_MULTIMODAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b9290b2ec9b76",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd912cb01d3abcc0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change physionet_username to your username\n",
    "if DOWNLOAD_DATASET:\n",
    "\n",
    "    physionet_username = \"your_user_name\"\n",
    "    password = \"your_pass\"\n",
    "    destination_directory = \"data/MIMICIII_Original\"\n",
    "\n",
    "    command = [\n",
    "        \"wget\", \"-r\", \"-N\", \"-c\", \"-np\",\n",
    "        \"--user\", physionet_username,\n",
    "        \"--password\", password,\n",
    "        \"https://physionet.org/files/mimiciii/1.4/\",\n",
    "        \"-P\", destination_directory\n",
    "    ]\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "\n",
    "    process.wait()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7101051b10d00a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if EXTRACT_COMPRESSED_CSVS:\n",
    "    command = ['./decompress_mimic.sh', '-d', 'data/MIMICIII_Original/physionet.org/files/mimiciii/1.4/', '-o',\n",
    "               'data/mimic3']\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "\n",
    "    process.wait()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee902ed9e8ef43",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preparing the data\n",
    "The original paper leverages the following projects to help on the data preparation and extraction from the original MIMIC CSVs\n",
    "\n",
    "It leverages the **mimic3-benchmarks** and the **ClinicalNotesICU** for the following:\n",
    "\n",
    "- Cleanup invalid data\n",
    "- Map the events, diagnoses, and stays for each patient.\n",
    "- Extract timeseries for in-hospital-mortality \n",
    "- Split timeseries data into train and test sets\n",
    "- Extract Medical notes for patients\n",
    "- Split Medical notes for train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63cceb7ae9be99",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MIMIC benchmarks\n",
    "Helps to process timeseries data and divide train and test sets\n",
    "[mimic3-benchmarks](https://github.com/YerevaNN/mimic3-benchmarks.git)\n",
    "\n",
    "This repo contains a set of scripts that take the RAW mimic CSVs and prepare the irregular data:\n",
    "- extract_subjects.py:\n",
    "Generates one directory per SUBJECT_ID and writes ICU stay information to data/{SUBJECT_ID}/stays.csv, diagnoses to data/{SUBJECT_ID}/diagnoses.csv, and events to data/{SUBJECT_ID}/events.csv\n",
    "\n",
    "\n",
    "- validate_events.py\n",
    "Attempts to fix some issues (ICU stay ID is missing) and removes the events that have missing information. About 80% of events remain after removing all suspicious rows\n",
    "\n",
    "\n",
    "- extract_episodes_from_subjects.py\n",
    "Breaks up per-subject data into separate episodes (pertaining to ICU stays). Time series of events are stored in {SUBJECT_ID}/episode{#}_timeseries.csv (where # counts distinct episodes) while episode-level information (patient age, gender, ethnicity, height, weight) and outcomes (mortality, length of stay, diagnoses) are stores in {SUBJECT_ID}/episode{#}.csv. This script requires two files, one that maps event ITEMIDs to clinical variables and another that defines valid ranges for clinical variables\n",
    "\n",
    "\n",
    "- split_train_and_test.py\n",
    "Splits the whole dataset into training and testing sets.\n",
    "\n",
    "\n",
    "- create_in_hospital_mortality.py\n",
    "Generate task-specific datasets for in-hospital-mortality prediction\n",
    "\n",
    "After running the preparation scripts we end up with a directory data/in-hospital-mortality we have two subdirectories: train and test. Each of them contains a bunch of ICU stays and one file with name listfile.csv, which lists all samples in that particular set. Each row of listfile.csv has the following form: icu_stay, period_length, label(s). A row specifies a sample for which the input is the collection of ICU event of icu_stay that occurred in the first period_length hours of the stay and the target are label(s). In in-hospital mortality prediction task period_length is always 48 hours.\n",
    "\n",
    "\n",
    "The project does not work out of the box, so we downloaded the sourcecode and modify it inside this repo under the [mimic3-benchmarks](./mimic3-benchmarks) folder\n",
    "To simplify the process we have created the following script `./build_benchmark_data.sh` to run all timeseries required tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e454792dbbde4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if PREPROCESS_BENCHMARKS:\n",
    "    command = [\"./build_benchmark_data.sh\"]\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "    process.wait()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f45bd0bc84d872",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ClinicalNotesICU\n",
    "Helps to process medical notes and divide in train and test\n",
    " [ClinicalNotesICU](https://github.com/kaggarwal/ClinicalNotesICU.git)\n",
    "\n",
    "Similar to the mimic3-benchmarks, this repo contains a set of scripts that take the RAW mimic CSVs and process the clinical notes for the previously generated train and test datasets.\n",
    "\n",
    "- extract_notes.py\n",
    "Uses the NOTEEVENTS.csv and the previously generated train and test sets to extract the notes within the first 48 hours of the event and saves them on its own train a test data directories \n",
    "\n",
    "- extract_T0.py\n",
    "Uses the stays.csv and events.csv to extract the episodes start time and save them into a binary pkl file.\n",
    "\n",
    "The project does not work out of the box, so we downloaded the sourcecode and modify it inside this repo under the [ClinicalNotesICU](./ClinicalNotesICU) folder\n",
    "\n",
    "To simplify the process we have created the following script `./extract_med_notes.sh` tu run all the required tasks for clinical notes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23722350fecd73f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if PREPROCESS_CLINICAL_NOTES:\n",
    "    command = [\"./extract_med_notes.sh\"]\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "    process.wait()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839edc8722b2b0e5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocess time series For Mimic Multimodal \n",
    "The next step is to discretize and normalize the timeseries data, as well as link the clinical notes with their corresponding timestamps.\n",
    "\n",
    "The [paper's repo](https://github.com/XZhang97666/MultimodalMIMIC.git) provides a preprocessing script to work on this task.\n",
    "\n",
    "After running the preprocessing steps we save the following PKLs to be used by the model: \n",
    "```\n",
    "mean_std.pkl \n",
    "norm_ts_test.pkl\n",
    "norm_ts_train.pkl\n",
    "norm_ts_val.pkl\n",
    "testp2x_data.pkl\n",
    "trainp2x_data.pkl\n",
    "ts_test.pkl\n",
    "ts_train.pkl\n",
    "ts_val.pkl\n",
    "valp2x_data.pkl\n",
    "```\n",
    "The project does not work out of the box, so we used it partially to call some of the functions from this notebook; the downloaded sourcecode and modifications are inside this repo under the [MultimodalMIMIC](./MultimodalMIMIC) folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d88a4406872a51",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Configuring imports and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09559d5533221a4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# Define ExperimentClass, this will help us to easy change the parameters and run multiple experiments reusing the same code\n",
    "# as well as keep track and save the results\n",
    "@dataclass\n",
    "class ModelExperiment:\n",
    "    tag: str\n",
    "    epochs: int = 1\n",
    "    max_text_length: int = 512\n",
    "    n_samples: Optional[int] = 500  # None for full dataset\n",
    "    imputation: str = \"previous\"  # 'zero', 'normal_value', 'previous', 'next'\n",
    "    training_time_millis: Optional[int] = None\n",
    "    eval_auc_results: Optional[dict] = None\n",
    "    eval_f1_results: Optional[dict] = None\n",
    "    eval_auprc_results: Optional[dict] = None\n",
    "\n",
    "\n",
    "# Define predefined experiments, the naming convention is as follows: experiment{epochs}e{n_samples}s{imputation_strategy}\n",
    "experiment1e1000sP = ModelExperiment(tag=\"experiment1e1000sP\", epochs=1, n_samples=1000, imputation=\"previous\")\n",
    "experiment3e1000sP = ModelExperiment(tag=\"experiment3e1000sP\", epochs=3, n_samples=1000, imputation=\"previous\")\n",
    "experiment6e1000sP = ModelExperiment(tag=\"experiment6e1000sP\", epochs=6, n_samples=1000, imputation=\"previous\")\n",
    "experiment1e3000sP = ModelExperiment(tag=\"experiment1e3000sP\", epochs=1, n_samples=3000, imputation=\"previous\")\n",
    "experiment3e3000sP = ModelExperiment(tag=\"experiment3e3000sP\", epochs=3, n_samples=3000, imputation=\"previous\")\n",
    "experiment6e3000sP = ModelExperiment(tag=\"experiment6e3000sP\", epochs=6, n_samples=3000, imputation=\"previous\")\n",
    "experiment1eFullP = ModelExperiment(tag=\"experiment1eFullP\", epochs=1, n_samples=None, imputation=\"previous\")\n",
    "experiment6eFullP = ModelExperiment(tag=\"experiment6eFullP\", epochs=6, n_samples=None, imputation=\"previous\")\n",
    "experiment3e3000sZ = ModelExperiment(tag=\"experiment3e3000sZ\", epochs=3, n_samples=3000, imputation=\"zero\")\n",
    "experiment6e1000sZ = ModelExperiment(tag=\"experiment6e1000sZ\", epochs=6, n_samples=1000, imputation=\"zero\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed034beed47eeaf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mimic3benchmark.readers import InHospitalMortalityReader\n",
    "from MultimodalMIMIC.preprocessing import Discretizer_multi\n",
    "from typing import Optional, Any\n",
    "from readers import Reader\n",
    "import gzip\n",
    "from mimic3models.preprocessing import Normalizer\n",
    "from MultimodalMIMIC.preprocessing import extract_irregular\n",
    "from MultimodalMIMIC.preprocessing import mean_std\n",
    "from MultimodalMIMIC.preprocessing import normalize\n",
    "from MultimodalMIMIC.text_utils import TextReader\n",
    "from MultimodalMIMIC.preprocessing import merge_text_ts\n",
    "\n",
    "GENERATE_PREPROCESSED_PKL = False # Change to True if the preprocessed pkl files are not available\n",
    "\n",
    "# Paths for data\n",
    "ihm_data_path = f\"{BENCHMARKS_ROOT_PATH}/data/in-hospital-mortality\"\n",
    "ihm_train_data_path = f\"{ihm_data_path}/train\"\n",
    "ihm_test_data_path = f\"{ihm_data_path}/test\"\n",
    "discretizer_config_path = f\"{MULTI_MODAL_MIMIC_PATH}/Data/irregular/discretizer_config.json\"\n",
    "channel_info_path = f\"{MULTI_MODAL_MIMIC_PATH}/Data/irregular/channel_info.json\"\n",
    "textdata_fixed = f\"{BENCHMARKS_ROOT_PATH}/data/root/text_fixed/train/\"\n",
    "text_start_time_path = f\"{BENCHMARKS_ROOT_PATH}/data/root/text_fixed/starttime.pkl\"\n",
    "test_textdata_fixed = f\"{BENCHMARKS_ROOT_PATH}/data/root/text_fixed/test/\"\n",
    "test_text_start_time_path = f\"{BENCHMARKS_ROOT_PATH}/data/root/text_fixed/test_starttime.pkl\"\n",
    "\n",
    "current_experiment = experiment1e1000sP\n",
    "sample_postfix = str(current_experiment.n_samples) if current_experiment.n_samples else \"Full\"\n",
    "ihm_discrete_save_path = f\"{MULTI_MODAL_MIMIC_PATH}/Data/ihm_{sample_postfix}_{current_experiment.imputation}\"\n",
    "output_path = f\"ihm_{sample_postfix}_{current_experiment.imputation}\"\n",
    "\n",
    "# Modify this to take only a subset of the full data; None takes the full data\n",
    "n_samples_elements = current_experiment.n_samples\n",
    "\n",
    "mortality_period = 48\n",
    "timestep = 1.0\n",
    "imputation = \"previous\"\n",
    "dataset_types = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "normalizer_state_file_path = f'{BENCHMARKS_ROOT_PATH}/mimic3models/in_hospital_mortality/ihm_ts{timestep}.input_str-previous.start_time-zero.normalizer'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972d061613ed073",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Reading the data\n",
    "First define the InHospitalMortalityReader to help us easily access the split datasets generated by mimic3-benchmarks. It provides helper functions to read multiple data series samples and labels by patient and map to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0db8b42d1f1ab5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if GENERATE_PREPROCESSED_PKL:\n",
    "    print(ihm_train_data_path)\n",
    "    train_reader = InHospitalMortalityReader(dataset_dir=ihm_train_data_path,\n",
    "                                             listfile=os.path.join(ihm_train_data_path, 'listfile.csv'),\n",
    "                                             period_length=mortality_period)\n",
    "    val_reader = InHospitalMortalityReader(dataset_dir=ihm_train_data_path,\n",
    "                                           listfile=os.path.join(ihm_train_data_path, 'listfile.csv'),\n",
    "                                           period_length=mortality_period)\n",
    "\n",
    "    test_reader = InHospitalMortalityReader(dataset_dir=ihm_test_data_path,\n",
    "                                            listfile=os.path.join(ihm_test_data_path, 'listfile.csv'),\n",
    "                                            period_length=mortality_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05696cbeab2d89a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_compressed_pkl_gz(data_to_dump: Any, save_name: str):\n",
    "    print(\"saving and compressing:\", save_name)\n",
    "    with gzip.open(f'{save_name}.pkl.gz', 'wb') as file:\n",
    "        pickle.dump(data_to_dump, file)\n",
    "\n",
    "\n",
    "def load_compressed_pkl(full_file_name: str) -> Any:\n",
    "    with gzip.open(full_file_name, 'rb') as file:\n",
    "        data_loaded = pickle.load(file)\n",
    "    return data_loaded\n",
    "\n",
    "\n",
    "def read_chunk(reader, chunk_size):\n",
    "    chunk_data = {}\n",
    "    for _ in tqdm(range(chunk_size), desc=\"reading data\"):\n",
    "        ret = reader.read_next()\n",
    "        for k, v in ret.items():\n",
    "            if k not in chunk_data:\n",
    "                chunk_data[k] = []\n",
    "            chunk_data[k].append(v)\n",
    "    chunk_data[\"header\"] = chunk_data[\"header\"][0]\n",
    "    return chunk_data\n",
    "\n",
    "\n",
    "def discretize_and_save_data(reader: Reader, discretizer: Discretizer_multi, save_path: str,\n",
    "                             partial_n_samples: Optional[int] = None, save_name=str, compress_pkl: bool = False):\n",
    "    n_samples = reader.get_number_of_examples()\n",
    "    if partial_n_samples:\n",
    "        n_samples = partial_n_samples\n",
    "    ret = read_chunk(reader, n_samples)\n",
    "    irg_data = ret[\"X\"]\n",
    "    ts = ret[\"t\"]\n",
    "    labels = ret[\"y\"]\n",
    "    discrete_names = ret[\"name\"]\n",
    "\n",
    "    reg_data = []\n",
    "    for X, t in tqdm(zip(irg_data, ts), total=len(irg_data), desc=f\"discretizing data \"):\n",
    "        transformed_data = discretizer.transform(X, end=t)[0]\n",
    "        reg_data.append(transformed_data)\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    save_full_path = f\"{save_path}/{save_name}\"\n",
    "    if compress_pkl:\n",
    "        save_compressed_pkl_gz((irg_data, reg_data, labels, discrete_names), save_full_path)\n",
    "    else:\n",
    "        print(\"Saving\", f\"{save_full_path}.pkl\")\n",
    "        with open(f\"{save_full_path}.pkl\", 'wb') as file:\n",
    "            pickle.dump((irg_data, reg_data, labels, discrete_names), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba45dd4edd24c95",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Discretize temporal data and add imputation\n",
    "\n",
    "In order to create the embeddings for the proposed model first we need to discretize and add the imputation to the missing values. In our case we will use imputation using the previous value of the series.\n",
    "\n",
    "The Discretizer_multi takes care of transforming the irregular data into samples at each timestep while filling the missing data with the desired imputation strategy considering all time based features.\n",
    "\n",
    "![Discretizer image](.img/only_discretize.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be5a812eb8f9ec",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if GENERATE_PREPROCESSED_PKL:\n",
    "    discretizer_multi = Discretizer_multi(\n",
    "        impute_strategy=current_experiment.imputation,\n",
    "        store_masks=True,\n",
    "        start_time='zero',\n",
    "        config_path=discretizer_config_path,\n",
    "        channel_path=channel_info_path\n",
    "    )\n",
    "    discretizer_header = discretizer_multi.transform(train_reader.read_example(0)[\"X\"])[1].split(',')\n",
    "    cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "    print(\"discretize and save train\")\n",
    "    discretize_and_save_data(train_reader, discretizer_multi, ihm_discrete_save_path, n_samples_elements, \"ts_train\")\n",
    "    print(\"discretize and save val\")\n",
    "    discretize_and_save_data(val_reader, discretizer_multi, ihm_discrete_save_path, n_samples_elements, \"ts_val\")\n",
    "    print(\"discretize and save test\")\n",
    "    discretize_and_save_data(test_reader, discretizer_multi, ihm_discrete_save_path, n_samples_elements, \"ts_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f129af1f21d395",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Paddings and masks\n",
    "After discretize we need to apply paddings and create masks for all features. For this we can use the function extract_irregular from MultimodalMimic. It creates the padded irregular and mask arrays and save them for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72092e5bc118117e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if GENERATE_PREPROCESSED_PKL:\n",
    "    for dataset_type in dataset_types:\n",
    "        print(f\"Extracting {dataset_type} irregular data\", flush=True)\n",
    "        in_extract_data_path = f\"{ihm_discrete_save_path}/ts_{dataset_type}.pkl\"\n",
    "        out_extract_data_path = f\"{ihm_discrete_save_path}/ts_{dataset_type}.pkl\"\n",
    "        extract_irregular(in_extract_data_path, out_extract_data_path, channel_info_path, discretizer_config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce6019f9e94910",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if GENERATE_PREPROCESSED_PKL:\n",
    "    mean_std(f\"{ihm_discrete_save_path}/ts_train.pkl\", f\"{ihm_discrete_save_path}/mean_std.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407668b846604ce6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Normalizing timeseries data\n",
    "Now we apply normalization to our data by x = (x - means[f_idx]) / stds[f_idx], For this, we leverage the Normalizer provided by the mimic3-benchmarks repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d118f0c2f7865",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if GENERATE_PREPROCESSED_PKL:\n",
    "    normalizer = Normalizer(fields=cont_channels)\n",
    "    normalizer.load_params(normalizer_state_file_path)\n",
    "\n",
    "    for dataset_type in dataset_types:\n",
    "        print(f\"Normalizing {dataset_type} times data\", flush=True)\n",
    "        normalize(f\"{ihm_discrete_save_path}/ts_{dataset_type}.pkl\",\n",
    "                  f\"{ihm_discrete_save_path}/norm_ts_{dataset_type}.pkl\",\n",
    "                  f\"{ihm_discrete_save_path}/mean_std.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef855063840a6659",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Preparing the Text data\n",
    "We finally prepare the text data within a period (48 in our case) and generate a json containing the note as well as the time until the end of the period. For this we use the TextReader helper from the MultimodalMIMIC module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae28b8cd4f293522",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if GENERATE_PREPROCESSED_PKL:\n",
    "    for dataset_type in dataset_types:\n",
    "        print(f\"Preparing  {dataset_type} text data\", flush=True)\n",
    "\n",
    "        with open(f\"{ihm_discrete_save_path}/norm_ts_{dataset_type}.pkl\", 'rb') as f:\n",
    "            tsdata = pickle.load(f)\n",
    "\n",
    "        names = [data['name'] for data in tsdata]\n",
    "\n",
    "        if (dataset_type == 'train') or (dataset_type == 'val'):\n",
    "            text_reader = TextReader(textdata_fixed, text_start_time_path)\n",
    "        else:\n",
    "            text_reader = TextReader(test_textdata_fixed, test_text_start_time_path)\n",
    "\n",
    "        data_text, data_times, data_time = text_reader.read_all_text_append_json(names, mortality_period)\n",
    "        merge_text_ts(data_text, data_times, data_time, tsdata, mortality_period,\n",
    "                      f\"{ihm_discrete_save_path}/{dataset_type}p2x_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645f45acb9458e6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Preprocessing results\n",
    "After running the preprocessing we will obtain the pkl files that will be used by the model for training and evaluation. The following cell will show the structure of the main pkl files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c7556d383c71f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_p2x_pkl_structure():\n",
    "    print(\"Structure of the p2x pkl files\")\n",
    "    with open(f\"{ihm_discrete_save_path}/testp2x_data.pkl\", 'rb') as file:\n",
    "        p2x_data = pickle.load(file)\n",
    "        print(f\"Keys p2x: {p2x_data[0].keys()}\")\n",
    "\n",
    "    with open(f\"{ihm_discrete_save_path}/norm_ts_test.pkl\", 'rb') as file:\n",
    "        p2x_data = pickle.load(file)\n",
    "        print(f\"Keys norm: {p2x_data[0].keys()}\")\n",
    "\n",
    "    with open(f\"{ihm_discrete_save_path}/mean_std.pkl\", 'rb') as file:\n",
    "        p2x_data = pickle.load(file)\n",
    "        print(f\"Keys mean_std tuple type : ({type(p2x_data[0])},{type(p2x_data[0])})\")\n",
    "\n",
    "\n",
    "display_p2x_pkl_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea129f0ca6817b1b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742c8dbf677eb5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import required modules and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define helper functions to save and load experiments\n",
    "\n",
    "def save_experiment(experiment: ModelExperiment, filename: str):\n",
    "    # Convert the dataclass to a dictionary\n",
    "    experiment_dict = asdict(experiment)\n",
    "\n",
    "    directory = os.path.dirname(filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            experiments = json.load(f)\n",
    "    else:\n",
    "        experiments = []\n",
    "\n",
    "    experiments.append(experiment_dict)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(experiments, f, indent=4)\n",
    "\n",
    "def load_experiment(filename: str):\n",
    "    with open(filename) as f:\n",
    "        experiment_dict = json.load(f)\n",
    "        experiment = ModelExperiment(**experiment_dict[0])\n",
    "        return experiment\n",
    "\n",
    "def load_all_experiments(filename: str):\n",
    "    with open(filename) as f:\n",
    "        experiments_dict = json.load(f)\n",
    "        experiments = [ModelExperiment(**experiment) for experiment in experiments_dict]\n",
    "        return experiments\n",
    "    \n",
    "def save_eval_results(eval_result: dict, experiment: ModelExperiment, path: str = f\"{PROJECT_BASE_PATH}/eval_results/experiments.json\"):\n",
    "    results = list(eval_result.values())[0]\n",
    "    experiment.eval_auc_results = results[\"auc\"]\n",
    "    experiment.eval_f1_results = results[\"f1\"]\n",
    "    experiment.eval_auprc_results = results[\"auprc\"]\n",
    "    save_experiment(experiment, path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27a45b87beed42b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0d798e3e52fc7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "sys.path.insert(0, 'MultimodalMIMIC')\n",
    "from GlobalConfigs import *\n",
    "from MultimodalMIMIC.model import *\n",
    "from MultimodalMIMIC.train import *\n",
    "from MultimodalMIMIC.checkpoint import *\n",
    "from accelerate import Accelerator\n",
    "from MultimodalMIMIC.interp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6062fd6a39c7d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Set arguments\n",
    "\n",
    "This step is to set up parameter to set up how to train and evaluate the model.\n",
    "\n",
    "depending on the experiment defined above, the below code will set up the training params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe52b12edd5709",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MultimodalMIMIC.util import parse_args\n",
    "\n",
    "parser = parse_args()\n",
    "args = parser.parse_args(['--num_train_epochs', f'{current_experiment.epochs}',\n",
    "                          '--train_batch_size', '2',\n",
    "                          '--eval_batch_size', '8',\n",
    "                          '--gradient_accumulation_steps', '16',\n",
    "                          '--num_update_bert_epochs', '2',\n",
    "                          '--notes_order', 'Last',\n",
    "                          '--max_length', f'{current_experiment.max_text_length}',\n",
    "                          '--output_dir', f'run/TS_Text/{output_path}',\n",
    "                          '--embed_dim', '128',\n",
    "                          '--model_name', 'bioLongformer',\n",
    "                          '--file_path', f'{ihm_discrete_save_path}',\n",
    "                          '--mixup_level', 'batch',\n",
    "                          '--fp16',\n",
    "                          '--irregular_learn_emb_text',\n",
    "                          '--irregular_learn_emb_ts',\n",
    "                          '--reg_ts'])\n",
    "\n",
    "print(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36408e5fb15eac2f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load model\n",
    "\n",
    "load model from model python.\n",
    "for full model implementation, check GitHub.\n",
    "\n",
    "based on given argument model type, the below code will load one of two models.\n",
    "\n",
    "- MULTCrossModel:\n",
    "  - This is a multi-modal cross model. It combines both text and time series data. Depending on the configuration, it may employ Transformer encoders for processing time series data and apply attention mechanisms for processing text embeddings. The model integrates text and time series data at different levels using various fusion techniques such as self-cross attention or cross-modal fusion. Again, the output depends on the task (ihm or pheno), and appropriate loss functions are used accordingly.\n",
    "- \n",
    "TSMixed\n",
    "  - This is a mixed model for time series data. It combines interpolation techniques, such as S_Interp and Cross_Interp, with Transformer-based encoders or other models like LSTM or CNN for processing time series data. It handles mixed-level data, such as batch-level, sequence-level, or feature-level mixup, and outputs predictions based on the task (ihm or pheno)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17d700490ade34",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![high level arch with desc](.img/high_arch_w_desc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814a16c38a9b49c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model(args, device, BioBert):\n",
    "    if 'Text' in args.modeltype:\n",
    "        return MULTCrossModel(args=args, device=device, orig_d_ts=17, orig_reg_d_ts=34, orig_d_txt=768,\n",
    "                               ts_seq_num=args.tt_max, text_seq_num=args.num_of_notes, Biobert=BioBert)\n",
    "    else:\n",
    "        return TSMixed(args=args, device=device, orig_d_ts=17, orig_reg_d_ts=34, ts_seq_num=args.tt_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c72c29e3d47c1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train\n",
    "\n",
    "This part of code train model, set up optimizer, prepare training environment and run the training process. \n",
    "Given modeltype argument from above, it will set optimizer for different model type. (Text, Timeseries or mix)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from MultimodalMIMIC.data import data_perpare\n",
    "from MultimodalMIMIC.util import loadBert\n",
    "import time\n",
    "\n",
    "\n",
    "# eval_result = eval_test(args, model, test_data_loader, device)\n",
    "\n",
    "def prepare_trining_eval(args):\n",
    "    \"\"\"\n",
    "    This function encasulates the training process plus the setup from the previous cells to be reused in later experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up training environment based on given argument above\n",
    "\n",
    "    if args.fp16:\n",
    "        args.mixed_precision = \"fp16\"\n",
    "    else:\n",
    "        args.mixed_precision = \"no\"\n",
    "    accelerator = Accelerator(mixed_precision=args.mixed_precision, cpu=args.cpu)\n",
    "\n",
    "    device = accelerator.device\n",
    "    print(f'device: {device}')\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    if args.tensorboard_dir != None:\n",
    "        writer = SummaryWriter(args.tensorboard_dir)\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "    make_save_dir(args)\n",
    "\n",
    "    if args.seed == 0:\n",
    "        copy_file(args.ck_file_path + 'model/', src=os.getcwd())\n",
    "\n",
    "    # Load training, validation and test dataset.\n",
    "    if args.mode == 'train':\n",
    "        if 'Text' in args.modeltype:\n",
    "            BioBert, BioBertConfig, tokenizer = loadBert(args, device)\n",
    "        else:\n",
    "            BioBert, tokenizer = None, None\n",
    "        train_dataset, train_sampler, train_dataloader = data_perpare(args, 'train', tokenizer)\n",
    "        val_dataset, val_sampler, val_dataloader = data_perpare(args, 'val', tokenizer)\n",
    "        _, _, test_data_loader = data_perpare(args, 'test', tokenizer)\n",
    "\n",
    "    # get model\n",
    "    model = get_model(args, device, BioBert)\n",
    "\n",
    "    #define optimizer\n",
    "    if args.modeltype == 'TS':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.ts_learning_rate)\n",
    "    elif args.modeltype == 'Text' or args.modeltype == 'TS_Text':\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': [p for n, p in model.named_parameters() if 'bert' not in n]},\n",
    "            {'params': [p for n, p in model.named_parameters() if 'bert' in n], 'lr': args.txt_learning_rate}\n",
    "        ], lr=args.ts_learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown modeltype in optimizer.\")\n",
    "\n",
    "    model, optimizer, train_dataloader, val_dataloader, test_data_loader = \\\n",
    "        accelerator.prepare(model, optimizer, train_dataloader, val_dataloader, test_data_loader)\n",
    "\n",
    "    return model, optimizer, train_dataloader, val_dataloader, test_data_loader, device, accelerator, writer\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model, optimizer, train_dataloader, val_dataloader, test_data_loader, device, accelerator, writer = prepare_trining_eval(\n",
    "    args)\n",
    "\n",
    "trainer_irg(model=model, args=args, accelerator=accelerator, train_dataloader=train_dataloader,\n",
    "            dev_dataloader=val_dataloader, test_data_loader=test_data_loader, device=device,\n",
    "            optimizer=optimizer, writer=writer)\n",
    "\n",
    "end_time = time.time()\n",
    "current_experiment.training_time_millis = (end_time - start_time) * 1000\n",
    "print(f\"Training time: {current_experiment.training_time_millis} ms\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b96c260de7b2cce",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14bec66f34abf3c5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab0794ced7c8d7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import asdict\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "eval_result = eval_test(args, model, test_data_loader, device)\n",
    "save_eval_results(eval_result, current_experiment)\n",
    "\n",
    "end_time = time.time()\n",
    "current_experiment.training_time_millis = (end_time - start_time) * 1000\n",
    "print(f\"Evaluation time: {current_experiment.training_time_millis} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6425b8566db17",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load evaluation results\n",
    "\n",
    "The performance of proposed methods and baselines are measured by the F1, AUPR, and AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea38aee8aebeaf7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Experiment eval result: {current_experiment.tag} - {current_experiment.eval_f1_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56f98a4705268e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiment Results \n",
    "\n",
    "Since for the demonstration we only used 1000 sample for 1 epoch, and we fill in dummy data for entry with missing value, we could not achieve the same result as the paper describe.\n",
    "\n",
    "We plan to further furnish the code and run on the full sample data with more epoch, to see if we could get the result as the paper describe.\n",
    "\n",
    "Table of results (no need to include additional experiments, but main reproducibility result should be included)\n",
    "Create table and graph comparing different params (n epochs, etc.)\n",
    "All claims should be supported by experiment results\n",
    "Discuss with respect to the hypothesis and results from the original paper\n",
    "Experiments beyond the original paper\n",
    "Each experiment should include results and a discussion\n",
    "Ablation Study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609fd938109d1e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Comparison between Hyperparameters\n",
    "\n",
    "| Experiment         | N samples    | Epochs | Imputation |\n",
    "|--------------------|--------------|--------|------------|\n",
    "| experiment1e1000sP | 1000 samples | 1      | previous   |\n",
    "| experiment3e1000sP | 1000 samples | 3      | previous   |\n",
    "| experiment6e1000sP | 1000 samples | 6      | previous   |\n",
    "| experiment1e3000sP | 3000 samples | 1      | previous   |\n",
    "| experiment3e3000sP | 3000 samples | 3      | previous   |\n",
    "| experiment6e3000sP | 3000 samples | 6      | previous   |\n",
    "| experiment1eFullP  | Full Dataset | 1      | previous   |\n",
    "| experiment6eFullP  | Full Dataset | 6      | previous   |\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from adjustText import adjust_text\n",
    "\n",
    "def experiments_to_dataframe(experiments) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for exp in experiments:\n",
    "        n_samples_label = \"1000\" if exp.n_samples == 1000 else \"3000\" if exp.n_samples == 3000 else \"Full_dataset\"\n",
    "        for res_type in [\"auc\", \"f1\", \"auprc\"]:\n",
    "            res_dict = getattr(exp, f\"eval_{res_type}_results\")\n",
    "            if res_dict:\n",
    "                rows.append({\n",
    "                    \"Tag\": exp.tag,\n",
    "                    \"Epoch\": exp.epochs,\n",
    "                    \"Type\": res_type.upper(),\n",
    "                    \"Val\": res_dict[\"val\"],\n",
    "                    \"Test\": res_dict[\"test\"],\n",
    "                    \"Dataset_Size\": n_samples_label,\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def plot_comparison_chart(df: pd.DataFrame, score_type: str):\n",
    "    # Subset for the given score type\n",
    "    df_sub = df[(df[\"Type\"] == score_type) & (df[\"Epoch\"] > 0)]\n",
    "\n",
    "    # Define a dictionary mapping dataset sizes to specific colors\n",
    "    dataset_colors = {\n",
    "        \"1000\": (\"blue\", \"cyan\"),\n",
    "        \"3000\": (\"green\", \"lightgreen\"),\n",
    "        \"Full_dataset\": (\"red\", \"pink\"),\n",
    "    }\n",
    "\n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    texts = []\n",
    "\n",
    "    for dataset in df_sub[\"Dataset_Size\"].unique():\n",
    "        df_ds = df_sub[df_sub[\"Dataset_Size\"] == dataset]\n",
    "\n",
    "        val_color, test_color = dataset_colors[dataset]\n",
    "        plt.scatter(df_ds[\"Epoch\"], df_ds[\"Val\"], label=f\"{dataset} (Val) {score_type}\", color=val_color)\n",
    "        plt.scatter(df_ds[\"Epoch\"], df_ds[\"Test\"], label=f\"{dataset} (Test) {score_type}\", color=test_color)\n",
    "\n",
    "        # Collect tags as annotations\n",
    "        for i, row in df_ds.iterrows():\n",
    "            texts.append(plt.text(row[\"Epoch\"], row[\"Val\"], row[\"Tag\"], fontsize=8, ha=\"right\", color=val_color))\n",
    "            texts.append(plt.text(row[\"Epoch\"], row[\"Test\"], row[\"Tag\"], fontsize=8, ha=\"right\", color=test_color))\n",
    "\n",
    "    adjust_text(texts, pull_threshold=2, arrowprops=dict(arrowstyle=\"->\", color='r', lw=0.5))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(f\"{score_type} Scores Across Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(f\"{score_type} Score\")\n",
    "    plt.xticks(df_sub[\"Epoch\"].unique())\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Load and process the experiments\n",
    "experiments = load_all_experiments(f\"{PROJECT_BASE_PATH}/eval_results/experiments.json\")\n",
    "\n",
    "# Remove elements which tag ends in Z\n",
    "experiments = [exp for exp in experiments if not exp.tag.endswith(\"Z\")]\n",
    "# Remove repeated tags\n",
    "seen_tags = {}\n",
    "filtered_experiments = []\n",
    "for exp in experiments:\n",
    "    if exp.tag not in seen_tags:\n",
    "        seen_tags[exp.tag] = True\n",
    "        filtered_experiments.append(exp)\n",
    "\n",
    "df = experiments_to_dataframe(filtered_experiments)\n",
    "\n",
    "# Plot each chart, val and test combined\n",
    "plot_comparison_chart(df, \"AUC\")\n",
    "plot_comparison_chart(df, \"F1\")\n",
    "plot_comparison_chart(df, \"AUPRC\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "beed7704647bc198",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ablation study\n",
    "\n",
    "\n",
    "For our project we plan to reproduce the experiment with In Hospital Mortality (IHM). \n",
    "In our ablation study, we want to find out \n",
    "- if the inclusion of imputation with UTDE improves the performance of the model.\n",
    "- if the introduction of mTand improves the performance of the model.\n",
    "\n",
    "Thus we plan to run training and evaluation on the below three control/test groups:\n",
    "\n",
    "1. Control group\n",
    "- Time series: \"previous\" imputation + mTand(ts) + UTDE gate\n",
    "- Text: Textencoder + mTand(txt)\n",
    "- multimodal fusion\n",
    "\n",
    "<!-- 2. Test1: Drop imputation by setting missing value to zero to prove the benefit of the imputation and UTDE approach.\n",
    "- Time series: \"zero\" imputation + mTand(ts) + UTDE gate\n",
    "- Text: Textencoder + mTand(txt)\n",
    "- multimodal fusion\n",
    "MULTCrossModel false true -->\n",
    "\n",
    "3. Test2: Drop mTAND from processing to prove the impact of considering irregularity during embedding.\n",
    "- Time series: \"previous\" imputation + UTDE gate\n",
    "- Text: Textencoder\n",
    "- multimodal fusion\n",
    " MULTCrossModel false false\n",
    "\n",
    "### Evaluation metrics:\n",
    "- AUPR\n",
    "- AUROC\n",
    "- F1\n",
    "\n",
    "### Shared data and hyper parameter \n",
    "- 48 hour IHM data\n",
    "- number of sample data: 3000\n",
    "- training epoch: 6"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f745df1fa319b567"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_and_eval(experiment, args):\n",
    "    if args.fp16:\n",
    "        args.mixed_precision = \"fp16\"\n",
    "    else:\n",
    "        args.mixed_precision = \"no\"\n",
    "    accelerator = Accelerator(mixed_precision=args.mixed_precision, cpu=args.cpu)\n",
    "\n",
    "    device = accelerator.device\n",
    "    print(f'device: {device}')\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    if args.tensorboard_dir != None:\n",
    "        writer = SummaryWriter(args.tensorboard_dir)\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "    output_path = make_save_dir(args)\n",
    "\n",
    "    if args.seed == 0:\n",
    "        copy_file(args.ck_file_path + 'model/', src=os.getcwd())\n",
    "\n",
    "    # load data\n",
    "\n",
    "    from MultimodalMIMIC.data import data_perpare\n",
    "    from MultimodalMIMIC.util import loadBert\n",
    "\n",
    "    if args.mode == 'train':\n",
    "            if 'Text' in args.modeltype:\n",
    "                BioBert, BioBertConfig, tokenizer = loadBert(args, device)\n",
    "            else:\n",
    "                BioBert, tokenizer = None, None\n",
    "            train_dataset, train_sampler, train_dataloader = data_perpare(args, 'train', tokenizer)\n",
    "            val_dataset, val_sampler, val_dataloader = data_perpare(args, 'val', tokenizer)\n",
    "            _, _, test_data_loader = data_perpare(args, 'test', tokenizer)\n",
    "\n",
    "    # load model\n",
    "    if 'Text' in args.modeltype:\n",
    "        model = MULTCrossModel(args=args, device=device, orig_d_ts=17, orig_reg_d_ts=34, orig_d_txt=768,\n",
    "                            ts_seq_num=args.tt_max, text_seq_num=args.num_of_notes, Biobert=BioBert)\n",
    "    else:\n",
    "        model = TSMixed(args=args, device=device, orig_d_ts=17, orig_reg_d_ts=34, ts_seq_num=args.tt_max)\n",
    "\n",
    "    # train model\n",
    "    if args.modeltype == 'TS':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=args.ts_learning_rate)\n",
    "    elif args.modeltype == 'Text' or args.modeltype == 'TS_Text':\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': [p for n, p in model.named_parameters() if 'bert' not in n]},\n",
    "            {'params': [p for n, p in model.named_parameters() if 'bert' in n], 'lr': args.txt_learning_rate}\n",
    "        ], lr=args.ts_learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown modeltype in optimizer.\")\n",
    "\n",
    "    model, optimizer, train_dataloader, val_dataloader, test_data_loader = \\\n",
    "        accelerator.prepare(model, optimizer, train_dataloader, val_dataloader, test_data_loader)\n",
    "\n",
    "    trainer_irg(model=model, args=args, accelerator=accelerator, train_dataloader=train_dataloader, \\\n",
    "                dev_dataloader=val_dataloader, test_data_loader=test_data_loader, device=device, \\\n",
    "                optimizer=optimizer, writer=writer)\n",
    "\n",
    "    # eval model\n",
    "    eval_result = eval_test(args, model, test_data_loader, device)\n",
    "\n",
    "    for result_file in os.listdir(args.ck_file_path):\n",
    "        if 'result.pkl' in result_file:\n",
    "            eval_result_path = args.ck_file_path + result_file\n",
    "            # print(eval_result_path)\n",
    "            with open(eval_result_path,'rb') as f:\n",
    "                evaluation_result = pickle.load(f)\n",
    "                print(evaluation_result)\n",
    "\n",
    "    return eval_result\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5faf9b97269f38a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def set_experiment(current_experiment, mtand, fusion_strategy='interleaved'):\n",
    "    sample_postfix = str(current_experiment.n_samples) if current_experiment.n_samples else \"Full\"\n",
    "    ihm_discrete_save_path = f\"{MULTI_MODAL_MIMIC_PATH}/Data/ihm_{sample_postfix}_{current_experiment.imputation}\"\n",
    "    output_path = f\"ihm_{sample_postfix}_{current_experiment.imputation}\"\n",
    "\n",
    "    parser = parse_args()\n",
    "\n",
    "    args = parser.parse_args(['--num_train_epochs', f'{current_experiment.epochs}',\n",
    "                              '--train_batch_size', '2',\n",
    "                              '--eval_batch_size', '8',\n",
    "                              '--gradient_accumulation_steps', '16',\n",
    "                              '--num_update_bert_epochs', '2',\n",
    "                              '--notes_order', 'Last',\n",
    "                              '--max_length', f'{current_experiment.max_text_length}',\n",
    "                              '--output_dir', f'run/TS_Text/{output_path}',\n",
    "                              '--embed_dim', '128',\n",
    "                              '--model_name', 'bioLongformer',\n",
    "                              '--file_path', f'{ihm_discrete_save_path}',\n",
    "                              '--mixup_level', 'batch',\n",
    "                              '--fp16',\n",
    "                              '--reg_ts'])\n",
    "    if mtand:\n",
    "        args.irregular_learn_emb_text = True\n",
    "        args.irregular_learn_emb_ts = True\n",
    "\n",
    "    if fusion_strategy != 'interleaved':\n",
    "        args.self_cross = False\n",
    "        args.cross_method = fusion_strategy\n",
    "\n",
    "    return args\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "235c5610df7702a8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Control\n",
    "\n",
    "Train and evaluate model follow the original set up from paper.\n",
    "- Time series: \"previous\" imputation and mTand(ts), combined with UTDE\n",
    "- Clinical notes: text encoder + mTand(txt)\n",
    "- Combined result of above two with MultiModal fusion \n",
    "\n",
    "Skip training and eval since we have run this setting in previous training section.\n",
    "Will just load eval result from file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5af1e0a30e6b1b93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "experiment_control = load_experiment(\"eval_results/experiment_control.json\")\n",
    "print(experiment_control)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c2d086038c64281",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test1: Use MAGgate fusion instead of Interleave fusion\n",
    "\n",
    "Train and evaluate model using \"MAGgate\" fusion strategy\n",
    "- Time series: \"previous\" imputation and mTand(ts), combined with UTDE\n",
    "- Clinical notes: text encoder + mTand(txt)\n",
    "- Combined result of above two with MAGgate fusion"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6acd7e9f4e208283"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "experiment_test1 = ModelExperiment(tag=\"experiment6e3000sP\", epochs=1, n_samples=3000, imputation=\"previous\")\n",
    "args = set_experiment(experiment_test1, mtand=True, fusion_strategy='MAGgate')\n",
    "print(vars(args))\n",
    "eval_result = train_and_eval(experiment_test1, args)\n",
    "save_eval_results(eval_result, experiment_test1, f\"{PROJECT_BASE_PATH}/eval_results/experiment_test1.json\")\n",
    "print(experiment_test1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a255d723e905515",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test2: Drop mTAND from processing to prove the impact of considering irregularity during embedding.\n",
    "\n",
    "Train and evaluate model dropping \"mTand\" module.\n",
    "- Time series: \"previous\" imputation\n",
    "- Clinical notes: text encoder\n",
    "- Combined result of above two with MultiModal fusion\n",
    "\n",
    "Drop mTand by set these arguments:\n",
    "- 'irregular_learn_emb_ts': False, \n",
    "- 'irregular_learn_emb_text': False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a0f7f94a0bfec25"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "experiment_test2 = ModelExperiment(tag=\"experiment6e3000sP\", epochs=1, n_samples=3000, imputation=\"previous\")\n",
    "args = set_experiment(experiment_test2, mtand=False)\n",
    "print(vars(args))\n",
    "eval_result = train_and_eval(experiment_test2, args)\n",
    "save_eval_results(eval_result, experiment_test2, f\"{PROJECT_BASE_PATH}/eval_results/experiment_test2.json\")\n",
    "print(experiment_test2)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17f28fb88df5c0b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from py_markdown_table.markdown_table import markdown_table\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "experiment_control = load_experiment(\"eval_results/experiment_control.json\")\n",
    "experiment_test1 = load_experiment(\"eval_results/experiment_test1.json\")\n",
    "experiment_test2 = load_experiment(\"eval_results/experiment_test2.json\")\n",
    "\n",
    "data = [\n",
    "    {\"test group\": \"Control\",\n",
    "     \"val accuracy\": round(experiment_control.eval_auc_results['val'],4),\n",
    "     \"val AUPRC\": round(experiment_control.eval_auprc_results['val'],4),\n",
    "     \"val F1\": round(experiment_control.eval_f1_results['val'],4),\n",
    "     \"test accuracy\": round(experiment_control.eval_auc_results['test'],4),\n",
    "     \"test AUPRC\": round(experiment_control.eval_auprc_results['test'],4),\n",
    "     \"test F1\": round(experiment_control.eval_f1_results['test'],4)},\n",
    "\n",
    "     {\"test group\": \"Test1: use MAGgate fusion instead of interleaved fusion\",\n",
    "     \"val accuracy\": round(experiment_test1.eval_auc_results['val'],4),\n",
    "     \"val AUPRC\": round(experiment_test1.eval_auprc_results['val'],4),\n",
    "     \"val F1\": round(experiment_test1.eval_f1_results['val'],4),\n",
    "     \"test accuracy\": round(experiment_test1.eval_auc_results['test'],4),\n",
    "     \"test AUPRC\": round(experiment_test1.eval_auprc_results['test'],4),\n",
    "     \"test F1\": round(experiment_test1.eval_f1_results['test'],4)},\n",
    "\n",
    "     {\"test group\": \"Test2: drop mTand\",\n",
    "     \"val accuracy\": round(experiment_test2.eval_auc_results['val'],4),\n",
    "     \"val AUPRC\": round(experiment_test2.eval_auprc_results['val'],4),\n",
    "     \"val F1\": round(experiment_test2.eval_f1_results['val'],4),\n",
    "     \"test accuracy\": round(experiment_test2.eval_auc_results['test'],4),\n",
    "     \"test AUPRC\": round(experiment_test2.eval_auprc_results['test'],4),\n",
    "     \"test F1\": round(experiment_test2.eval_f1_results['test'],4)},\n",
    "]\n",
    "\n",
    "markdown = markdown_table(data).get_markdown()\n",
    "display(Markdown(markdown))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55305e78edf3fb78"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ablation conclusion\n",
    "\n",
    "Given the above ablation study experiment results, we can see that:\n",
    "- the use of interleaved fusion has better performance than MAGgate in some metrics. The use of interleaved fusion produce better test AUPRC, but didn't have better test F1 result as described in paper.\n",
    "- the use of mTand definitely has better performance metric than not using mTand. As the one uses mTand has better test accurancy, AUPRC and F1 than the one doesn't.\n",
    "\n",
    "In conclusion, we can tell the introduction of mTAND for processing time series and clinical notes has produce better AUPRC and F1 score against standard baselines.\n",
    "But the introduce of interleaved fusion does not reproduce the result in the paper. This may due to us not using the full sample of data due to computation power limit.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "685a1b8ea514efe9"
  },
  {
   "cell_type": "markdown",
   "id": "3042596241658c7c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Discussion and learnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e17be980100ea1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Implications of the experimental results\n",
    "The results of our experiments show the benefits of using the UTDE, mTAND and the cross modal fusion improves the performance of the model based on the F1 score. However, given the Hardware and time limitations we were unable to 100% reproduce the scores obtained by the authors. Specifically we used a lower amount of epochs for the training that have a direct impact on the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67733eb99f6aeb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### What was easy\n",
    "The paper itself was very easy to understand on the theoretical side, the explanations for UTDE and mTAND concepts were very well explained and easy to understand. On the code side, even though it was not focussed on reproducibility we were able to reuse some of the code from the original authors and adapt it to our needs, specially the Model structure and some of the preprocessing steps. Also, as in the original paper, the mimic3-benchmarks and ClinicalNotesICU repos were very helpful to preprocess the original CSV datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e10a1c91d01d3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Challenges\n",
    "The main challenges we faced for the reproducibility of the paper were as follows:\n",
    "- The dependencies for the project were not documented or not included in the requirements.txt file. \n",
    "    - We manually had to check the code and the requirements.txt file to find the dependencies missing.\n",
    "- Requirements.txt file pointed to the local versions of the original authors and the versions were not compatible with the current versions of the libraries.\n",
    "    - We tested multiple versions of the libraries to find the compatible versions and updated the requirements.txt file accordingly.\n",
    "- Some libraries used for the original project were not available anymore or were experimental versions at the time of the original project implementation.\n",
    "    - We had to find an alternative version and syntax for the libraries for similar outcome. \n",
    "- The preprocessing steps were not straightforward, and were unable to process edge cases like missing values in the time series data or text data.\n",
    "    - We debug and  tested multiple approaches to fill or skip in the missing values and preprocess the data. We found the best outcome by filling the missing values with a \"NO_DATA\" token for text and time_to_end = 0 for time series data.\n",
    "- The original implementation did not take into account the different path formatting for different OS, which made it difficult to run the code on different OS.\n",
    "    - We introduced a GlobalConfigs.py file to set up the local path to the project and used it in the code to make it OS independent. \n",
    "- The configuration arguments for the original code were not documented and had not descriptive names and given the amount of data folders and pkls it was difficult to understand the code and requirements for each part.\n",
    "    - We added descriptive names to the configuration variables on GlobalConfigs.py as well as composing the subdirectories based on the relative paths.\n",
    "- There was no information about the repo version or commit hash used for the mimic3-benchmarks and ClinicalNotesICU repos, since the external projects changed the output formats and the file naming we had to update the code to make these inputs interchangeable.\n",
    "    - We modify the code to use the new file naming and output formats from the mimic3-benchmarks and ClinicalNotesICU repos and add variables to configure these files in the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20c815f0a61aad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Recommendations to the original authors\n",
    "Based on our experience, we recommend the following to the original authors for improving reproducibility:\n",
    "- Document the dependencies and the versions of the libraries used in the project.\n",
    "- Use the requirements.txt file to list all the dependencies and their versions making sure they are publicly available.\n",
    "- Document Hardware used for the experiments, as the project requires a lot of computational resources.\n",
    "- Document the configuration arguments used for the project and provide a description for each argument.\n",
    "- Provide the folder structure and the expected output on the environment where the project was developed.\n",
    "- Provide the version of the external projects used in the project and the commit hash for the external projects.\n",
    "- Provide sample input required for the model training and evaluation.\n",
    "- Provide details about the dataset and how to get access to it if it is not publicly available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c7f291f7d6a1d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
