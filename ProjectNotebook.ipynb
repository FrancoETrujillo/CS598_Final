{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f954d686b0a84023",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Improving Medical Predictions by Irregular Multimodal Electronic Health Records Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b40d117099b4b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Introduction\n",
    "This paper pretends to address the challenges of handling irregularity and the integration of multimodal data for medical prediction tasks.\n",
    "\n",
    "## Background of the problem\n",
    "### What type of problem:\n",
    "The paper focuses on 2 main problems; Mortality Prediction and Phenotype Classification\n",
    "### What is the importance/meaning of solving the problem: \n",
    "ICUs admit patients with life-threatening conditions, Improving the efficacy and efficiency of predictions by accounting for irregular data in EHRs can help the medical providers to make more accurate and quick decisions that could save lives.\n",
    "\n",
    "### What is the difficulty of the problem:\n",
    "The primary difficulty is the handling the irregular sampling of data and the effective integration and modeling of EHR records like numerical time series and textual notes taken in multiple points in time and frequencies.\n",
    "\n",
    "### The state of the art methods and effectiveness.\n",
    "For irregular data handling;\n",
    "> [1] Lipton, Z. C., Kale, D., and Wetzel, R. Directly modeling\n",
    "> missing data in sequences with rnns: Improved classification of clinical time series. In Machine learning for\n",
    "> healthcare conference, pp. 253–270. PMLR, 2016.\n",
    "\n",
    "> [2] Shukla, S. N. and Marlin, B. M. Multi-time attention networks for irregularly sampled time series. arXiv preprint\n",
    "> arXiv:2101.10318, 2021.\n",
    "\n",
    "For irregular clinical notes processing;\n",
    "> [3] Golmaei, S. N. and Luo, X. Deepnote-gnn: predicting hospital readmission using clinical notes and patient network.\n",
    "> In Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics,\n",
    "> pp. 1–9, 2021.\n",
    "\n",
    "> [4]Mahbub, M., Srinivasan, S., Danciu, I., Peluso, A., Begoli, E., Tamang, S., and Peterson, G. D. \n",
    "> Unstructured clinical notes within the 24 hours since admission predict short,> mid & long-term mortality in adult icu patients. \n",
    "> Plos one, 17(1):e0262182, 2022.\n",
    "\n",
    "## Paper explanation\n",
    "### What did the paper propose\n",
    "The general problem addressed in this paper is to find a better approach to handling irregular multimodal data obtained on EHRs to better assess real-time predictions in ICUs. \n",
    "\n",
    "### What is the innovations of the method\n",
    "To better approach irregularity and multi-modal data the paper proposes integrating the real-time series and clinical notes while considering their irregularities. This by doing the following:\n",
    "\n",
    "#### Modeling Irregularity in Time Series:\n",
    "1. Temporal Discretization-Based Embeddings (TDE): Utilizes a novel unified\n",
    "approach (UTDE) that combines:\n",
    "    - Imputation: Regularizes time series by filling in missing values based\n",
    "on prior observations or statistical methods.\n",
    "    - Discretized Multi-Time Attention (mTAND): Applies a learned\n",
    "interpolation method using a multi-time attention mechanism to\n",
    "represent the irregular time series data better.\n",
    "2. Unified Approach (UTDE): This approach integrates imputation and mTAND\n",
    "through a gating mechanism to dynamically combine the representation of\n",
    "the time series. \n",
    "\n",
    "     \n",
    "#### Processing Irregular Clinical Notes:\n",
    "1. Text Encoding: Uses a pretrained model (TextEncoder) to encode clinical\n",
    "notes into a series of representations.\n",
    "2. Irregularity Modeling: Sorts these representations by time, treats them as\n",
    "Multivariate Irregularly Sampled Time Series (MINSTS), and employs mTAND\n",
    "to generate a set of text interpolation representations to handle irregularities.\n",
    "\n",
    " \n",
    "#### Multimodal Fusion:\n",
    "1. Interleaved Attention Mechanism: Fuses time series and clinical note\n",
    "representations across temporal steps, integrating irregularity into multimodal\n",
    "representations.\n",
    "2. Self and Cross-Attention:\n",
    "    - Multi-Head Self-Attention (MH): Acquires contextual embeddings for\n",
    "each modality by focusing within the same modality across time.\n",
    "    - Multi-Head Cross-Attention (CMH): Each modality learns from the\n",
    "other, integrating information across modalities.\n",
    "3. Feed-Forward and Prediction Layers: A feed-forward sublayer follows the\n",
    "CMH outputs, with layer normalization and residual connections applied. The\n",
    "final step involves passing the integrated representations through fully\n",
    "connected layers to predict the outcome.\n",
    "\n",
    "### How well the proposed method work (in its own metrics)\n",
    " The proposed methods for two medical prediction tasks consistently outperforms state-ofthe-art (SOTA) baselines in each single modality and multimodal fusion scenarios. \n",
    "Observing a relative improvements of 6.5%, 3.6%, and 4.3% in F1 for time series, clinical notes, and multimodal fusion, respectively. \n",
    "\n",
    "### What is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n",
    "The paper's contribution is important because it provides a new direction for EHR-based predictive models to consider time irregularity that could lead to more accurate and reliable medical predictions, helping patients and healthcare processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091ff27ef282204",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "For our project we plan to reproduce the experiment with In Hospital Mortality (IHM). And prove the following hypotheses:\n",
    "\n",
    "\n",
    "1. The inclusion of UTDE improves the performance of the model.\n",
    "2. Considering irregularities in clinical note embedding improves the performance of the model.\n",
    "3. The introduction of UTDE and mTAND for processing time series and clinical notes, respectively, plus the integration of Multimodal fusion outperforms F1 score against standard baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d958f689e7c76",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Methodology\n",
    "\n",
    "The project reproduction consists on the following sections\n",
    "- Data\n",
    "- Models\n",
    "- Training\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fccbbee93bdeb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "This paper uses the MIMICIII dataset as starting point to obtain timeseries information and medical notes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea0197a327d87e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " \n",
    "## Getting the data\n",
    " The following code contains some useful code to download and extract the dataset files locally\n",
    " \n",
    "**Note**: To download the mimic dataset is necessary to complete the request for access at [Physionet](https://physionet.org/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55643c225c7bfa5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Configs\n",
    "Open and edit GlobalConfigs.py to set up the local path to the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e41a54007f74873",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports and configs\n",
    "import subprocess\n",
    "import pickle\n",
    "import os\n",
    "from GlobalConfigs import *\n",
    "\n",
    "DOWNLOAD_DATASET = False\n",
    "EXTRACT_COMPRESSED_CSVS = False\n",
    "PREPROCESS_BENCHMARKS = False\n",
    "PREPROCESS_CLINICAL_NOTES = False\n",
    "PREPROCESS_MULTIMODAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd912cb01d3abcc0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change physionet_username to your username\n",
    "if DOWNLOAD_DATASET:\n",
    "   \n",
    "    physionet_username = \"ftrujillo\"\n",
    "    password = \"\"\n",
    "    destination_directory = \"data/MIMICIII_Original\"\n",
    "    \n",
    "    command = [\n",
    "        \"wget\", \"-r\", \"-N\", \"-c\", \"-np\",\n",
    "        \"--user\", physionet_username,\n",
    "        \"--password\", password,\n",
    "        \"https://physionet.org/files/mimiciii/1.4/\",\n",
    "        \"-P\", destination_directory\n",
    "    ]\n",
    "    \n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    \n",
    "    # for line in process.stdout:\n",
    "        # print(line, end='')\n",
    "    \n",
    "    process.wait()\n",
    "    \n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b7101051b10d00a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if EXTRACT_COMPRESSED_CSVS:\n",
    "    command = ['./decompress_mimic.sh', '-d', 'data/MIMICIII_Original/physionet.org/files/mimiciii/1.4/', '-o', 'data/mimic3']\n",
    "    \n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    \n",
    "    for line in process.stdout:\n",
    "        print(line, end='')   \n",
    "        \n",
    "    process.wait()\n",
    "    \n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee902ed9e8ef43",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Preparing the data\n",
    "The original paper leverages the following projects to help on the data preparation and extraction from the original MIMIC CSVs\n",
    "\n",
    "It leverages the **mimic3-benchmarks** and the **ClinicalNotesICU** for the following:\n",
    "\n",
    "- Cleanup invalid data\n",
    "- Map the events, diagnoses, and stays for each patient.\n",
    "- Extract timeseries for in-hospital-mortality \n",
    "- Split timeseries data into train and test sets\n",
    "- Extract Medical notes for patients\n",
    "- Split Medical notes for train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63cceb7ae9be99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### MIMIC benchmarks\n",
    "Helps to process timeseries data and divide train and test sets\n",
    "[mimic3-benchmarks](https://github.com/YerevaNN/mimic3-benchmarks.git)\n",
    "\n",
    "This repo contains a set of scripts that take the RAW mimic CSVs and prepare the irregular data:\n",
    "- extract_subjects.py:\n",
    "Generates one directory per SUBJECT_ID and writes ICU stay information to data/{SUBJECT_ID}/stays.csv, diagnoses to data/{SUBJECT_ID}/diagnoses.csv, and events to data/{SUBJECT_ID}/events.csv\n",
    "\n",
    "\n",
    "- validate_events.py\n",
    "Attempts to fix some issues (ICU stay ID is missing) and removes the events that have missing information. About 80% of events remain after removing all suspicious rows\n",
    "\n",
    "\n",
    "- extract_episodes_from_subjects.py\n",
    "Breaks up per-subject data into separate episodes (pertaining to ICU stays). Time series of events are stored in {SUBJECT_ID}/episode{#}_timeseries.csv (where # counts distinct episodes) while episode-level information (patient age, gender, ethnicity, height, weight) and outcomes (mortality, length of stay, diagnoses) are stores in {SUBJECT_ID}/episode{#}.csv. This script requires two files, one that maps event ITEMIDs to clinical variables and another that defines valid ranges for clinical variables\n",
    "\n",
    "\n",
    "- split_train_and_test.py\n",
    "Splits the whole dataset into training and testing sets.\n",
    "\n",
    "\n",
    "- create_in_hospital_mortality.py\n",
    "Generate task-specific datasets for in-hospital-mortality prediction\n",
    "\n",
    "After running the preparation scripts we end up with a directory data/in-hospital-mortality we have two subdirectories: train and test. Each of them contains a bunch of ICU stays and one file with name listfile.csv, which lists all samples in that particular set. Each row of listfile.csv has the following form: icu_stay, period_length, label(s). A row specifies a sample for which the input is the collection of ICU event of icu_stay that occurred in the first period_length hours of the stay and the target are label(s). In in-hospital mortality prediction task period_length is always 48 hours.\n",
    "\n",
    "\n",
    "The project does not work out of the box, so we downloaded the sourcecode and modify it inside this repo under the [mimic3-benchmarks](./mimic3-benchmarks) folder\n",
    "To simplify the process we have created the following script `./build_benchmark_data.sh` to run all timeseries required tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "572e454792dbbde4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T01:47:36.786602047Z",
     "start_time": "2024-04-11T01:47:36.760296724Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if PREPROCESS_BENCHMARKS:\n",
    "    command = [\"./build_benchmark_data.sh\"]\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "    \n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "    process.wait()\n",
    "    \n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f45bd0bc84d872",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### ClinicalNotesICU\n",
    "Helps to process medical notes and divide in train and test\n",
    " [ClinicalNotesICU](https://github.com/kaggarwal/ClinicalNotesICU.git)\n",
    "\n",
    "Similar to the mimic3-benchmarks, this repo contains a set of scripts that take the RAW mimic CSVs and process the clinical notes for the previously generated train and test datasets.\n",
    "\n",
    "- extract_notes.py\n",
    "Uses the NOTEEVENTS.csv and the previously generated train and test sets to extract the notes within the first 48 hours of the event and saves them on its own train a test data directories \n",
    "\n",
    "- extract_T0.py\n",
    "Uses the stays.csv and events.csv to extract the episodes start time and save them into a binary pkl file.\n",
    "\n",
    "The project does not work out of the box, so we downloaded the sourcecode and modify it inside this repo under the [ClinicalNotesICU](./ClinicalNotesICU) folder\n",
    "\n",
    "To simplify the process we have created the following script `./extract_med_notes.sh` tu run all the required tasks for clinical notes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d23722350fecd73f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if PREPROCESS_CLINICAL_NOTES:\n",
    "    command = [\"./extract_med_notes.sh\"]\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True)\n",
    "    \n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "    process.wait()\n",
    "    \n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839edc8722b2b0e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Preprocess time series and notes to create timestamps and text chunk PKLs\n",
    "The next step is to discretize and normalize the timeseries data, as well as link the clinical notes with their corresponding timestamps.\n",
    "\n",
    "The [paper's repo](https://github.com/XZhang97666/MultimodalMIMIC.git) provides a preprocessing script to take care of this task.\n",
    "\n",
    "After running the preprocessing script we obtain the following PKLs to be used by the model: \n",
    "```\n",
    "mean_std.pkl \n",
    "norm_ts_test.pkl\n",
    "norm_ts_train.pkl\n",
    "norm_ts_val.pkl\n",
    "testp2x_data.pkl\n",
    "trainp2x_data.pkl\n",
    "ts_test.pkl\n",
    "ts_train.pkl\n",
    "ts_val.pkl\n",
    "valp2x_data.pkl\n",
    "```\n",
    "The project does not work out of the box, so we downloaded the sourcecode and modify it inside this repo under the [MultimodalMIMIC](./MultimodalMIMIC) folder\n",
    "The pre-processing script is located at [MultimodalMIMIC/preprocessing.py](MultimodalMIMIC/preprocessing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f7990c4a9f0a349",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if PREPROCESS_MULTIMODAL:\n",
    "    command = [\"python\",\"-W\",\"ignore\", \"MultimodalMIMIC/preprocessing.py\"]\n",
    "\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    \n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "        \n",
    "    process.wait()\n",
    "    \n",
    "    if process.returncode != 0:\n",
    "        print(f\"Command failed with return code {process.returncode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361f279fd3d555d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Read preprocessed PKLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e7d3cd1d20c29c4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 42276\n",
      "train: 35948\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clinical_notes_path = f\"{BENCHMARKS_ROOT_PATH}/data/root/text_fixed\"\n",
    "\n",
    "# Start times pkl contains a map of patient_id notes with its recorded time\n",
    "test_note_start_time = f\"{clinical_notes_path}/test_starttime.pkl\"\n",
    "train_note_start_time = f\"{clinical_notes_path}/starttime.pkl\"\n",
    "\n",
    "test_start_times_dict = {}\n",
    "train_start_times_dict = {}\n",
    "with open(test_note_start_time, 'rb') as f:\n",
    "    test_start_times_dict.update(pickle.load(f))\n",
    "\n",
    "with open(train_note_start_time, 'rb') as f:\n",
    "    train_start_times_dict.update(pickle.load(f))\n",
    "\n",
    "print(\"test:\", len(test_start_times_dict))\n",
    "print(\"train:\", len(train_start_times_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa7eccdffa04299b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test texts: 6107\n",
      "train texts: 34748\n",
      "/Users/hongyiwu/code/CS598_Final/mimic3-benchmarks/data/root/text_fixed/test/25124_1\n"
     ]
    }
   ],
   "source": [
    "text_train_files = []\n",
    "text_test_files = []\n",
    "\n",
    "text_train_filepath = f\"{clinical_notes_path}/train\"\n",
    "text_test_filepath = f\"{clinical_notes_path}/test\"\n",
    "\n",
    "with os.scandir(text_train_filepath) as entries:\n",
    "    text_train_files.extend([entry.name for entry in entries if entry.is_file() and entry.name[0].isdigit()])\n",
    "\n",
    "with os.scandir(text_test_filepath) as entries:\n",
    "    text_test_files.extend([entry.name for entry in entries if entry.is_file() and entry.name[0].isdigit()])\n",
    "\n",
    "print(\"test texts:\", len(text_test_files))\n",
    "print(\"train texts:\", len(text_train_files))\n",
    "print(f\"{text_test_filepath}/{text_test_files[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "246aeb4f99d4a8d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /Users/hongyiwu/code/CS598_Final/MultimodalMIMIC/Data/ihm/trainp2x_data.pkl\n",
      "pkl data: dict_keys(['reg_ts', 'name', 'label', 'ts_tt', 'irg_ts', 'irg_ts_mask'])\n"
     ]
    }
   ],
   "source": [
    "# Open PKL\n",
    "dataPath = \"/Users/hongyiwu/code/CS598_Final/MultimodalMIMIC/Data/ihm/trainp2x_data.pkl\"\n",
    "if os.path.isfile(dataPath):\n",
    "    print('Using', dataPath)\n",
    "    with open(dataPath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        print(\"pkl data:\", data[0].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea129f0ca6817b1b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930d852c-8b89-45d1-9882-6f88877c546f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import dependency\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from MultimodalMIMIC.model import *\n",
    "from MultimodalMIMIC.train import *\n",
    "from MultimodalMIMIC.checkpoint import *\n",
    "from MultimodalMIMIC.util import *\n",
    "from accelerate import Accelerator\n",
    "from MultimodalMIMIC.interp import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d64d22c4-0ae7-4397-b3a2-1cc742be9670",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# set up input arguments\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m args\u001b[38;5;241m.\u001b[39ma \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "# set up input arguments\n",
    "\n",
    "args.a = \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e279c1a-73fb-48b3-8947-a2be125dad43",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'output_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(args\u001b[38;5;241m.\u001b[39moutput_dir)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'output_dir'"
     ]
    }
   ],
   "source": [
    "print(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fef23-de2a-4804-8cdd-78fb5149be04",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Decide what training should be conduct based on arguments\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "start_time = time.time()\n",
    "\n",
    "if args.fp16:\n",
    "    args.mixed_precision = \"fp16\"\n",
    "else:\n",
    "    args.mixed_precision = \"no\"\n",
    "accelerator = Accelerator(mixed_precision=args.mixed_precision, cpu=args.cpu)\n",
    "\n",
    "device = accelerator.device\n",
    "print(device)\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "if args.tensorboard_dir != None:\n",
    "    writer = SummaryWriter(args.tensorboard_dir)\n",
    "else:\n",
    "    writer = None\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "make_save_dir(args)\n",
    "\n",
    "if args.seed == 0:\n",
    "    copy_file(args.ck_file_path + 'model/', src=os.getcwd())\n",
    "if args.mode == 'train':\n",
    "    if 'Text' in args.modeltype:\n",
    "        BioBert, BioBertConfig, tokenizer = loadBert(args, device)\n",
    "    else:\n",
    "        BioBert, tokenizer = None, None\n",
    "    train_dataset, train_sampler, train_dataloader = data_perpare(args, 'train', tokenizer)\n",
    "    val_dataset, val_sampler, val_dataloader = data_perpare(args, 'val', tokenizer)\n",
    "    _, _, test_data_loader = data_perpare(args, 'test', tokenizer)\n",
    "\n",
    "if 'Text' in args.modeltype:\n",
    "    model = MULTCrossModel(args=args, device=device, orig_d_ts=17, orig_reg_d_ts=34, orig_d_txt=768,\n",
    "                           ts_seq_num=args.tt_max, text_seq_num=args.num_of_notes, Biobert=BioBert)\n",
    "else:\n",
    "    model = TSMixed(args=args, device=device, orig_d_ts=17, orig_reg_d_ts=34, ts_seq_num=args.tt_max)\n",
    "\n",
    "print(device)\n",
    "\n",
    "if args.modeltype == 'TS':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.ts_learning_rate)\n",
    "elif args.modeltype == 'Text' or args.modeltype == 'TS_Text':\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': [p for n, p in model.named_parameters() if 'bert' not in n]},\n",
    "        {'params': [p for n, p in model.named_parameters() if 'bert' in n], 'lr': args.txt_learning_rate}\n",
    "    ], lr=args.ts_learning_rate)\n",
    "else:\n",
    "    raise ValueError(\"Unknown modeltype in optimizer.\")\n",
    "\n",
    "model, optimizer, train_dataloader, val_dataloader, test_data_loader = \\\n",
    "    accelerator.prepare(model, optimizer, train_dataloader, val_dataloader, test_data_loader)\n",
    "\n",
    "trainer_irg(model=model, args=args, accelerator=accelerator, train_dataloader=train_dataloader, \\\n",
    "            dev_dataloader=val_dataloader, test_data_loader=test_data_loader, device=device, \\\n",
    "            optimizer=optimizer, writer=writer)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abaeb54759b9c2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26139a98645604c7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# eval result stats\n",
    "\n",
    "eval_test(args, model, test_data_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e085c337f3375",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Ablation 1 - Drop UTDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee0bc909bde8c6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50a006bd896d6e70",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Ablation 2 - Remove mTAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7df7867cf3098e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
